[
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "First, I will create a synthetic dataset with multiple variables and some associations between them. I will then explore the data with plots and tables, and fit some simple models to see if they can recover the associations.\n\nGenerate Synthetic Dataset\n\n# Load necessary libraries\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate synthetic dataset\nn &lt;- 1000\nx1 &lt;- rnorm(n, mean = 5, sd = 2)\nx2 &lt;- rnorm(n, mean = 3, sd = 1)\nx3 &lt;- rbinom(n, 1, 0.5)\ny &lt;- 3 + 2 * x1 - 1.5 * x2 + 0.8 * x3 + rnorm(n)\n\n# Create data frame\ndf &lt;- data.frame(y, x1, x2, x3)\n\n# Explore the dataset\nsummary(df)\n\n       y                x1                x2                 x3       \n Min.   :-2.994   Min.   :-0.6195   Min.   :-0.04786   Min.   :0.000  \n 1st Qu.: 6.049   1st Qu.: 3.7434   1st Qu.: 2.34678   1st Qu.:0.000  \n Median : 8.906   Median : 5.0184   Median : 3.05485   Median :0.000  \n Mean   : 8.893   Mean   : 5.0323   Mean   : 3.04246   Mean   :0.489  \n 3rd Qu.:11.557   3rd Qu.: 6.3292   3rd Qu.: 3.75345   3rd Qu.:1.000  \n Max.   :22.691   Max.   :11.4821   Max.   : 6.39037   Max.   :1.000  \n\nhead(df)\n\n          y       x1        x2 x3\n1  7.072988 3.879049 2.0042013  0\n2 10.513535 4.539645 1.9600450  1\n3 14.857289 8.117417 2.9820198  1\n4  9.246312 5.141017 2.8678249  1\n5 13.615550 5.258575 0.4506572  0\n6 13.323778 8.430130 4.0405735  0\n\n\n\n\nExplore the Data\n\nPlot the relationships between variables\n\n# Plot y against x1\nggplot(df, aes(x = x1, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Scatter plot of y vs x1\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Plot y against x2\nggplot(df, aes(x = x2, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Scatter plot of y vs x2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Boxplot of y by x3\nggplot(df, aes(x = as.factor(x3), y = y)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of y by x3\", x = \"x3\")\n\n\n\n\n\nScatter plot of y vs x1: This plot shows a positive linear relationship between x1 and y. As x1 increases, y also tends to increase. The fitted line, shown in blue, confirms this positive correlation.\nScatter plot of y vs x2: This plot shows a slight negative linear relationship between x2 and y. As x2 increases, y tends to decrease slightly. The fitted line, shown in blue, indicates this negative correlation.\nBoxplot of y by x3: This boxplot shows the distribution of y for two categories of x3 (0 and 1). The median y value appears to be slightly higher for x3 = 1 compared to x3 = 0, indicating a potential difference in y based on the category of x3.\n\nOverall, the plots confirm the expected relationships in the synthetic data, with x1 having a positive association with y, x2 having a negative association with y, and x3 showing some differences in y based on its category.\nNow I will fit three linear models to explore the relationships in the synthetic dataset.\n\n\n\nFit Simple Models\n\nLinear Model\n\n# Fit a linear model\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0422 -0.6685 -0.0369  0.6641  3.3416 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.00272    0.12866   23.34   &lt;2e-16 ***\nx1           1.98449    0.01603  123.83   &lt;2e-16 ***\nx2          -1.45520    0.03147  -46.24   &lt;2e-16 ***\nx3           0.67653    0.06333   10.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 996 degrees of freedom\nMultiple R-squared:  0.9436,    Adjusted R-squared:  0.9434 \nF-statistic:  5554 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nSummary:\n\nIntercept: 3.00272\nx1 coefficient: 1.98449\nx2 coefficient: -1.45520\nx3 coefficient: 0.67653\nMultiple R-squared: 0.9436\n\n\n\n\nInterpretation\n\nThe first model (linear) shows strong relationships with significant coefficients for all variables.\n\n\nExplore Model Coefficients\n\n# Print the coefficients of the linear model\ncat(\"Coefficients of the linear model:\\n\")\n\nCoefficients of the linear model:\n\nprint(coef(model))\n\n(Intercept)          x1          x2          x3 \n  3.0027214   1.9844910  -1.4551997   0.6765316 \n\n\n\n\n\nExplore Different Models\n\nPolynomial Regression\n\n# Fit a polynomial regression model\nmodel_poly &lt;- lm(y ~ poly(x1, 2) + poly(x2, 2) + x3, data = df)\nsummary(model_poly)\n\n\nCall:\nlm(formula = y ~ poly(x1, 2) + poly(x2, 2) + x3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0556 -0.6441 -0.0302  0.6627  3.3272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.56089    0.04433 193.105   &lt;2e-16 ***\npoly(x1, 2)1 124.40827    1.00551 123.726   &lt;2e-16 ***\npoly(x1, 2)2  -0.57825    1.00256  -0.577    0.564    \npoly(x2, 2)1 -46.44628    1.00515 -46.208   &lt;2e-16 ***\npoly(x2, 2)2  -0.22716    1.00122  -0.227    0.821    \nx3             0.67839    0.06346  10.690   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.001 on 994 degrees of freedom\nMultiple R-squared:  0.9436,    Adjusted R-squared:  0.9433 \nF-statistic:  3327 on 5 and 994 DF,  p-value: &lt; 2.2e-16\n\n\nSummary:\n\nIntercept: 8.56089\nx1 (1st term) coefficient: 124.40827\nx1 (2nd term) coefficient: -0.57825 (not significant)\nx2 (1st term) coefficient: -46.44628\nx2 (2nd term) coefficient: -0.22716 (not significant)\nx3 coefficient: 0.67839\nMultiple R-squared: 0.9436\n\n\n\n\nInterpretation\n\nThe second model (polynomial) adds quadratic terms for x1 and x2, but the additional complexity does not significantly improve the model, as the quadratic terms are not significant.\n\n\nInteraction Model\n\n# Fit a model with interaction terms\nmodel_interact &lt;- lm(y ~ x1 * x2 + x1 * x3 + x2 * x3, data = df)\nsummary(model_interact)\n\n\nCall:\nlm(formula = y ~ x1 * x2 + x1 * x3 + x2 * x3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0441 -0.6361 -0.0548  0.6617  3.3446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.67485    0.31034   8.619  &lt; 2e-16 ***\nx1           2.04367    0.05427  37.660  &lt; 2e-16 ***\nx2          -1.33398    0.09360 -14.251  &lt; 2e-16 ***\nx3           0.67439    0.24948   2.703  0.00698 ** \nx1:x2       -0.02194    0.01589  -1.381  0.16759    \nx1:x3        0.01434    0.03215   0.446  0.65569    \nx2:x3       -0.02219    0.06320  -0.351  0.72551    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.001 on 993 degrees of freedom\nMultiple R-squared:  0.9437,    Adjusted R-squared:  0.9434 \nF-statistic:  2775 on 6 and 993 DF,  p-value: &lt; 2.2e-16\n\n\nSummary:\n\nIntercept: 2.67485\nx1 coefficient: 2.04367\nx2 coefficient: -1.33398\nx3 coefficient: 0.67439\nx1\ninteraction: -0.02194 (not significant)\nx1\ninteraction: 0.01434 (not significant)\nx2\ninteraction: -0.02219 (not significant)\nMultiple R-squared: 0.9437\n\n\n\n\nInterpretation\n\nThe third model (interaction) introduces interaction terms, but they are not significant, indicating no substantial interaction effects between the variables.\n\n\n\nConclusion\nThis R code generates a synthetic dataset with multiple variables and known associations, explores the data with plots and tables, and fits several simple models to recover the associations. The linear model should reveal the coefficients that were built into the data generation process, while the polynomial and interaction models allow us to explore more complex relationships.\nOverall, the linear model successfully captures the main associations between the variables and y, while additional complexity from polynomial and interaction terms does not significantly enhance the model’s explanatory power."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Importing necessary libraries & packages\n\nlibrary(\"dslabs\") #loading dslabs package\nlibrary(\"tidyverse\") #loading tidyverse package\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"dplyr\") #loading dplyr package\nlibrary(\"ggplot2\") #loading ggplot2 package\ndata(\"gapminder\") #importing gapminder data\n\nThe dslabs package is an R package designed to provide datasets and functions for data science and statistics education. It includes a variety of real-world datasets that are useful for teaching and learning purposes.\nThe tidyverse is a collection of R packages designed for data science. All packages in the tidyverse share an underlying design philosophy, grammar, and data structures, which makes them work seamlessly together. The tidyverse includes packages for data manipulation, visualization, and modeling, and it’s particularly well-suited for data analysis tasks.\nKey packages in the tidyverse include:\n\nggplot2: For data visualization.\ndplyr: For data manipulation.\ntidyr: For data tidying.\nreadr: For data import.\npurrr: For functional programming.\ntibble: For modern data frames.\nstringr: For string manipulation.\nforcats: For working with categorical data (factors).\n\nThe dplyr package is a part of the tidyverse and provides a set of functions that are designed to simplify data manipulation in R. It is known for its ease of use and efficiency in handling data frames. Here are some key features and functions of the dplyr package:\n\nData Manipulation: dplyr allows you to perform a variety of data manipulation tasks, including filtering rows, selecting columns, reordering rows, and summarizing data.\nKey Functions:\n\nfilter(): Subsets rows based on their values.\nselect(): Subsets columns based on their names.\narrange(): Reorders rows.\nmutate(): Adds new columns or transforms existing ones.\nsummarize(): Aggregates data to produce a single summary statistic.\ngroup_by(): Groups data for grouped operations.\n\nChaining with Pipes: One of the most powerful features of dplyr is its compatibility with the pipe operator (%&gt;%). This allows you to chain multiple operations together in a clear and readable way.\nPerformance: dplyr functions are optimized for performance, making it efficient to work with large datasets.\nIntegration: dplyr integrates well with other tidyverse packages, making it easy to use in conjunction with packages like ggplot2 for visualization and tidyr for tidying data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "href": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Importing necessary libraries & packages\n\nlibrary(\"dslabs\") #loading dslabs package\nlibrary(\"tidyverse\") #loading tidyverse package\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"dplyr\") #loading dplyr package\nlibrary(\"ggplot2\") #loading ggplot2 package\ndata(\"gapminder\") #importing gapminder data\n\nThe dslabs package is an R package designed to provide datasets and functions for data science and statistics education. It includes a variety of real-world datasets that are useful for teaching and learning purposes.\nThe tidyverse is a collection of R packages designed for data science. All packages in the tidyverse share an underlying design philosophy, grammar, and data structures, which makes them work seamlessly together. The tidyverse includes packages for data manipulation, visualization, and modeling, and it’s particularly well-suited for data analysis tasks.\nKey packages in the tidyverse include:\n\nggplot2: For data visualization.\ndplyr: For data manipulation.\ntidyr: For data tidying.\nreadr: For data import.\npurrr: For functional programming.\ntibble: For modern data frames.\nstringr: For string manipulation.\nforcats: For working with categorical data (factors).\n\nThe dplyr package is a part of the tidyverse and provides a set of functions that are designed to simplify data manipulation in R. It is known for its ease of use and efficiency in handling data frames. Here are some key features and functions of the dplyr package:\n\nData Manipulation: dplyr allows you to perform a variety of data manipulation tasks, including filtering rows, selecting columns, reordering rows, and summarizing data.\nKey Functions:\n\nfilter(): Subsets rows based on their values.\nselect(): Subsets columns based on their names.\narrange(): Reorders rows.\nmutate(): Adds new columns or transforms existing ones.\nsummarize(): Aggregates data to produce a single summary statistic.\ngroup_by(): Groups data for grouped operations.\n\nChaining with Pipes: One of the most powerful features of dplyr is its compatibility with the pipe operator (%&gt;%). This allows you to chain multiple operations together in a clear and readable way.\nPerformance: dplyr functions are optimized for performance, making it efficient to work with large datasets.\nIntegration: dplyr integrates well with other tidyverse packages, making it easy to use in conjunction with packages like ggplot2 for visualization and tidyr for tidying data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#using-gapminder-dataset",
    "href": "coding-exercise/coding-exercise.html#using-gapminder-dataset",
    "title": "R Coding Exercise",
    "section": "Using Gapminder Dataset",
    "text": "Using Gapminder Dataset\nHelp() function to see what data contains\n\nhelp(gapminder) #looking at help file for gapminder data\n\nStr() and Summary() functions to take a look at the data\n\nstr(gapminder) #getting an overview of data structure\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n\n\nsummary(gapminder) #getting a summary of data\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n\nClass() function to check what type of object gapminder is\n\nclass(gapminder) #determining the type of object gapminder is\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing data",
    "text": "Processing data\nFilter the data: The filter function from dplyr is used to filter rows where the continent column equals “Africa”. The result is stored in the africadata object.\n\nafricadata &lt;- gapminder %&gt;% filter(continent == \"Africa\") #Filtering the data to include only African countries\n\nExplore africadata: We use the str and summary functions to display the structure and summary statistics of africadata.\n\nstr(africadata) #Displaying structure of the africadata object\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n\n\nsummary(africadata) #Displaying summary of the africadata object\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nCreate new objects: We create two new objects:\n\nafricadata_infant_life contains the infant_mortality and life_expectancy columns.\nafricadata_population_life contains the population and life_expectancy columns.\n\nExplore new objects: We use the str and summary functions to display the structure and summary statistics of both new objects.\n\nafricadata_infant_life &lt;- africadata %&gt;% select(infant_mortality, life_expectancy) #Creating a new object containing only 'infant_mortality' and 'life_expectancy'\n\nstr(africadata_infant_life) #Displaying structure of the new object containing only 'infant_mortality' and 'life_expectancy'\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\n\n\nsummary(africadata_infant_life) #Displaying summary of the new object containing only 'infant_mortality' and 'life_expectancy'\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\n\nafricadata_population_life &lt;- africadata %&gt;% select(population, life_expectancy) #Creating another new object containing only 'population' and 'life_expectancy'\n\nstr(africadata_population_life) #Displaying structure of the second new object containing only 'population' and 'life_expectancy'\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n\n\nsummary(africadata_population_life) #Displaying structure and summary of the second new object containing only 'population' and 'life_expectancy'\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting-to-analyze-key-relationships-in-african-countries-data",
    "href": "coding-exercise/coding-exercise.html#plotting-to-analyze-key-relationships-in-african-countries-data",
    "title": "R Coding Exercise",
    "section": "Plotting to Analyze Key Relationships in African Countries Data",
    "text": "Plotting to Analyze Key Relationships in African Countries Data\n\nplot1 &lt;- ggplot(africadata_infant_life, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color=\"darkgreen\") +\n  labs(title = \"Life Expectancy vs Infant Mortality in African countries\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\") +\n  theme_minimal() \n#Plotting life expectancy as a function of infant mortality for African countries\n\nprint(plot1)\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n#Displaying the first plot\n\nExplanation of the Observations\nLife Expectancy vs Infant Mortality:\n\nIn the first plot, we observe a negative correlation between infant mortality and life expectancy. This means that as infant mortality increases, life expectancy tends to decrease. This relationship makes sense because high infant mortality rates are often associated with poor healthcare, inadequate nutrition, and other socio-economic factors that negatively impact overall life expectancy.\nThe points are colored in dark green for better visual distinction.\n\n\nplot2 &lt;- ggplot(africadata_population_life, aes(x = population, y = life_expectancy)) +\n  geom_point(color=\"red\") +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs Population Size in African Countries\",\n       x = \"Population Size (log scale)\",\n       y = \"Life Expectancy\") +\n  theme_minimal()\n#Plotting life expectancy as a function of population size (x-axis in log scale) for African countries\n#Displaying the second plot\nprint(plot2)\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExplanation of the Observations\nLife Expectancy vs Population Size:\n\nIn the second plot, we observe a positive correlation between population size and life expectancy. This suggests that countries with larger populations tend to have higher life expectancy. One reason for this could be that larger populations often have better infrastructure, healthcare systems, and resources that contribute to higher life expectancy.\nThe plot uses a log scale for the x-axis to better visualize the wide range of population sizes. Using a log scale helps to spread out the data points and makes it easier to see patterns and relationships.\nThe points are colored in red for better visual distinction.\nWe also notice streaks of data that seem to group together. These streaks may indicate that there are clusters of countries with similar population sizes and life expectancy. Taking a closer look at the africadata dataset, we may find that these clusters represent countries with similar socio-economic conditions, healthcare access, and other factors that influence life expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-data-processing",
    "href": "coding-exercise/coding-exercise.html#more-data-processing",
    "title": "R Coding Exercise",
    "section": "More data processing",
    "text": "More data processing\nExtracting Data for Year 2000\n\nmissing_years &lt;- africadata[is.na(africadata$infant_mortality), \"year\"]\n#Identifying years with missing data for infant mortality\n\nIdentify Years with Missing Data:\n\nWe first identify the years with missing data for the infant_mortality variable by filtering rows where infant_mortality is NA.\nWe then get the unique years with missing data and print them to identify which years to avoid.\n\n\nunique_missing_years &lt;- unique(missing_years)\nprint(unique_missing_years)\n\n [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974\n[16] 1975 1976 1977 1978 1979 1980 1981 2016\n\n#Displaying unique years with missing data\n\n\nafricadata_2000 &lt;- africadata[africadata$year == 2000, ]\n#Extracting data for the year 2000\n\nExtract Data for the Year 2000:\n\nWe extract only the rows from africadata where the year is 2000.\nWe assign this filtered data to a new object called africadata_2000.\n\n\nstr(africadata_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n#Checking the structure of the new dataset\n\n\nsummary(africadata_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n#Checking the summary of the new dataset\n\nCheck Structure and Summary:\nWe use the str and summary functions to check the structure and summary statistics of the new dataset africadata_2000.\n\nThe new dataset should have 51 observations and 9 variables.\n\nBy extracting the data for the year 2000, we can now analyze the patterns without the confounding effects of different years. This allows us to focus on the relationship between variables for a single year, providing clearer insights into the data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting",
    "href": "coding-exercise/coding-exercise.html#more-plotting",
    "title": "R Coding Exercise",
    "section": "More plotting",
    "text": "More plotting\n\nafricadata_2000 &lt;- africadata[africadata$year == 2000, ]\n#Creating a new object with data for the year 2000\n\np_infant_mortality_2000 &lt;- ggplot(africadata_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"Life Expectancy vs. Infant Mortality (Year 2000) in African Countries\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\")\n#Plotting life expectancy as a function of infant mortality in African countries\n\nprint(p_infant_mortality_2000)\n\n\n\n#Displaying the plot\n\nLife Expectancy vs. Infant Mortality (Year 2000) in African Countries:\nThe scatter plot shows a negative correlation between infant mortality and life expectancy. As infant mortality decreases, life expectancy tends to increase. This makes sense as lower infant mortality rates often indicate better healthcare and living conditions, leading to longer life expectancy.\n\np_population_2000 &lt;- ggplot(africadata_2000, aes(x = population, y = life_expectancy)) +\n  geom_point(color = \"red\") +\n  scale_x_log10() +  # Set x-axis to log scale\n  labs(title = \"Life Expectancy vs. Population Size (Year 2000) in African Countries\",\n       x = \"Population (Log Scale)\",\n       y = \"Life Expectancy\")\n#Plotting life expectancy as a function of population size in African countries\n\nprint(p_population_2000)\n\n\n\n#Displaying the plot\n\nLife Expectancy vs. Population Size (Year 2000) in African Countries:\nThe scatter plot with the population size on a log scale shows no noticeable correlation between population size and life expectancy. This indicates that population size does not have a strong direct impact on life expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-model-fits",
    "href": "coding-exercise/coding-exercise.html#simple-model-fits",
    "title": "R Coding Exercise",
    "section": "Simple model fits",
    "text": "Simple model fits\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_2000)\n#Fitting the first model with life expectancy as the outcome and infant mortality as the predictor\n\nsummary_fit1 &lt;- summary(fit1)\n#Applying the summary command to the first fit\n\nprint(summary_fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\nExplanation and Interpretation\nModel 1: Life Expectancy vs. Infant Mortality\nFit 1 Summary:\nsummary_fit1\nThis command provides detailed statistics about the linear regression model. Key metrics include the coefficients, standard errors, t-values, and p-values for the predictor (infant_mortality) and the intercept. Additionally, it shows the R-squared value, which indicates the proportion of variance in the outcome (life_expectancy) explained by the predictor.\nInterpretation:\n\nThe p-value for the infant mortality predictor is likely to be very small (p-value &lt; 0.05), indicating a statistically significant relationship between infant mortality and life expectancy.\nThe negative coefficient for infant mortality suggests that as infant mortality increases, life expectancy decreases, which aligns with the negative correlation observed in the scatter plot.\n\n\nfit2 &lt;- lm(life_expectancy ~ (population), data = africadata_2000)\n#Fitting the second model with life expectancy as the outcome and population size as the predictor\n\nsummary_fit2 &lt;- summary(fit2)\n#Applying the summary command to the second fit\nprint(summary_fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ (population), data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nExplanation and Interpretation\nModel 2: Life Expectancy vs. Population Size\nFit 2 Summary:\nsummary_fit2\nThis command provides similar statistics for the linear regression model with population size as the predictor.\nInterpretation:\nThe p-value for the population predictor is likely to be large (p-value &gt; 0.05), indicating that there is no statistically significant relationship between population size and life expectancy.\nThe coefficient for the transformed population size may not provide a meaningful interpretation due to the lack of statistical significance."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#conclusion",
    "href": "coding-exercise/coding-exercise.html#conclusion",
    "title": "R Coding Exercise",
    "section": "Conclusion",
    "text": "Conclusion\nInfant Mortality as Predictor:\n\nThere is a statistically significant negative relationship between infant mortality and life expectancy. This suggests that higher infant mortality rates are associated with lower life expectancy.\n\nPopulation Size as Predictor:\n\nThere is no statistically significant relationship between population size and life expectancy. Population size does not appear to be a strong predictor of life expectancy based on the 2000 data.\n\nBy fitting these simple models and interpreting the results, we gain insights into the relationships between life expectancy, infant mortality, and population size. This process highlights the importance of using statistical models to understand data patterns and make informed conclusions."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-contributed-by-collin-real",
    "href": "coding-exercise/coding-exercise.html#this-section-contributed-by-collin-real",
    "title": "R Coding Exercise",
    "section": "This section contributed by Collin Real",
    "text": "This section contributed by Collin Real"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#explore-the-dataset",
    "href": "coding-exercise/coding-exercise.html#explore-the-dataset",
    "title": "R Coding Exercise",
    "section": "Explore the dataset",
    "text": "Explore the dataset\nStr() and Summary() functions to take a look at the data\n\ndata(polls_us_election_2016) # import data\n\n\nhelp(polls_us_election_2016)\n\n\nstr(polls_us_election_2016) #getting an overview of data structure\n\n'data.frame':   4208 obs. of  15 variables:\n $ state           : Factor w/ 57 levels \"Alabama\",\"Alaska\",..: 50 50 50 50 50 50 50 50 37 50 ...\n $ startdate       : Date, format: \"2016-11-03\" \"2016-11-01\" ...\n $ enddate         : Date, format: \"2016-11-06\" \"2016-11-07\" ...\n $ pollster        : Factor w/ 196 levels \"ABC News/Washington Post\",..: 1 63 81 194 65 55 18 113 195 76 ...\n $ grade           : Factor w/ 10 levels \"D\",\"C-\",\"C\",\"C+\",..: 10 6 8 6 5 9 8 8 NA 8 ...\n $ samplesize      : int  2220 26574 2195 3677 16639 1295 1426 1282 8439 1107 ...\n $ population      : chr  \"lv\" \"lv\" \"lv\" \"lv\" ...\n $ rawpoll_clinton : num  47 38 42 45 47 ...\n $ rawpoll_trump   : num  43 35.7 39 41 43 ...\n $ rawpoll_johnson : num  4 5.46 6 5 3 3 5 6 6 7.1 ...\n $ rawpoll_mcmullin: num  NA NA NA NA NA NA NA NA NA NA ...\n $ adjpoll_clinton : num  45.2 43.3 42 45.7 46.8 ...\n $ adjpoll_trump   : num  41.7 41.2 38.8 40.9 42.3 ...\n $ adjpoll_johnson : num  4.63 5.18 6.84 6.07 3.73 ...\n $ adjpoll_mcmullin: num  NA NA NA NA NA NA NA NA NA NA ...\n\n\n\nsummary(polls_us_election_2016) #getting a summary of data\n\n            state        startdate             enddate          \n U.S.          :1106   Min.   :2015-11-06   Min.   :2015-11-08  \n Florida       : 148   1st Qu.:2016-08-10   1st Qu.:2016-08-21  \n North Carolina: 125   Median :2016-09-23   Median :2016-09-30  \n Pennsylvania  : 125   Mean   :2016-08-31   Mean   :2016-09-06  \n Ohio          : 115   3rd Qu.:2016-10-20   3rd Qu.:2016-10-28  \n New Hampshire : 112   Max.   :2016-11-06   Max.   :2016-11-07  \n (Other)       :2477                                            \n                                     pollster        grade     \n Ipsos                                   : 919   A-     :1085  \n Google Consumer Surveys                 : 743   B      :1011  \n SurveyMonkey                            : 660   C-     : 693  \n YouGov                                  : 130   C+     : 329  \n Rasmussen Reports/Pulse Opinion Research: 125   B+     : 204  \n USC Dornsife/LA Times                   : 121   (Other): 457  \n (Other)                                 :1510   NA's   : 429  \n   samplesize       population        rawpoll_clinton rawpoll_trump  \n Min.   :   35.0   Length:4208        Min.   :11.04   Min.   : 4.00  \n 1st Qu.:  447.5   Class :character   1st Qu.:38.00   1st Qu.:35.00  \n Median :  772.0   Mode  :character   Median :43.00   Median :40.00  \n Mean   : 1148.2                      Mean   :41.99   Mean   :39.83  \n 3rd Qu.: 1236.5                      3rd Qu.:46.20   3rd Qu.:45.00  \n Max.   :84292.0                      Max.   :88.00   Max.   :68.00  \n NA's   :1                                                           \n rawpoll_johnson  rawpoll_mcmullin adjpoll_clinton adjpoll_trump   \n Min.   : 0.000   Min.   : 9.0     Min.   :17.06   Min.   : 4.373  \n 1st Qu.: 5.400   1st Qu.:22.5     1st Qu.:40.21   1st Qu.:38.429  \n Median : 7.000   Median :25.0     Median :44.15   Median :42.765  \n Mean   : 7.382   Mean   :24.0     Mean   :43.32   Mean   :42.674  \n 3rd Qu.: 9.000   3rd Qu.:27.9     3rd Qu.:46.92   3rd Qu.:46.290  \n Max.   :25.000   Max.   :31.0     Max.   :86.77   Max.   :72.433  \n NA's   :1409     NA's   :4178                                     \n adjpoll_johnson  adjpoll_mcmullin\n Min.   :-3.668   Min.   :11.03   \n 1st Qu.: 3.145   1st Qu.:23.11   \n Median : 4.384   Median :25.14   \n Mean   : 4.660   Mean   :24.51   \n 3rd Qu.: 5.756   3rd Qu.:27.98   \n Max.   :20.367   Max.   :31.57   \n NA's   :1409     NA's   :4178"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#pre-process-data",
    "href": "coding-exercise/coding-exercise.html#pre-process-data",
    "title": "R Coding Exercise",
    "section": "Pre Process Data",
    "text": "Pre Process Data\n\n# Filter by U.S. state greater than October 31, 2016 and remove null values from grade\ndf = polls_us_election_2016 |&gt; \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" &\n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade)))\nhead(df, 10)\n\n   state  startdate    enddate\n1   U.S. 2016-11-03 2016-11-06\n2   U.S. 2016-11-02 2016-11-06\n3   U.S. 2016-11-03 2016-11-06\n4   U.S. 2016-11-02 2016-11-06\n5   U.S. 2016-11-03 2016-11-05\n6   U.S. 2016-11-04 2016-11-07\n7   U.S. 2016-11-04 2016-11-06\n8   U.S. 2016-11-01 2016-11-04\n9   U.S. 2016-11-03 2016-11-06\n10  U.S. 2016-11-01 2016-11-03\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                                       Ipsos    A-       2195\n3  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n4                                     CBS News/New York Times    A-       1426\n5                                NBC News/Wall Street Journal    A-       1282\n6                                                    IBD/TIPP    A-       1107\n7                                            Selzer & Company    A+        799\n8                                           Angus Reid Global    A-       1151\n9                                         Monmouth University    A+        748\n10                                             Marist College     A        940\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv            47.0          43.0             4.0               NA\n2          lv            42.0          39.0             6.0               NA\n3          lv            48.0          44.0             3.0               NA\n4          lv            45.0          41.0             5.0               NA\n5          lv            44.0          40.0             6.0               NA\n6          lv            41.2          42.7             7.1               NA\n7          lv            44.0          41.0             4.0               NA\n8          lv            48.0          44.0             6.0               NA\n9          lv            50.0          44.0             4.0               NA\n10         lv            44.0          43.0             6.0               NA\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221               NA\n2         42.02638      38.81620        6.844734               NA\n3         49.02208      43.95631        3.057876               NA\n4         45.11649      40.92722        4.341786               NA\n5         43.58576      40.77325        5.365788               NA\n6         42.92745      42.23545        6.316175               NA\n7         44.21714      40.57082        4.068708               NA\n8         47.57171      43.68125        5.556625               NA\n9         48.86765      43.39600        4.838600               NA\n10        42.83406      43.43819        4.780429               NA"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#visualize-distribution-of-data",
    "href": "coding-exercise/coding-exercise.html#visualize-distribution-of-data",
    "title": "R Coding Exercise",
    "section": "Visualize Distribution of Data",
    "text": "Visualize Distribution of Data\n\nBoxplots - Trump Polls\n\nggplot(df, aes(x = grade, y = rawpoll_trump)) +\n  geom_boxplot(fill='steelblue', color='black') +\n  labs(title = \"Trump Boxplot - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n\n\n\n\n\nBoxplots - Clinton Polls\n\nggplot(df, aes(x = grade, y = rawpoll_clinton)) +\n  geom_boxplot(fill='steelblue', color='black') +\n  labs(title = \"Clinton Boxplot - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n\n\n\n\n\nHistogram - Trump Polls\n\ndf %&gt;%\n  ggplot(aes(x = rawpoll_trump)) +\n  geom_histogram(color='black', fill='steelblue') +\n  labs(title = \"Trump Histogram - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nHistogram - Hillary Polls\n\n# Create a boxplot of price distribution by electric vehicle type\ndf %&gt;%\n  select(grade, rawpoll_clinton) %&gt;%\n  ggplot(aes(x = rawpoll_clinton)) +\n  geom_histogram(color='black', fill='steelblue') +\n  labs(title = \"Trump Histogram - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nFound this snippet from an online tutorial\n\npolls_us_election_2016 %&gt;%\n  filter(state == \"U.S.\" & enddate&gt;=\"2016-07-01\") %&gt;%\n  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %&gt;%\n  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %&gt;%\n  gather(candidate, percentage, -enddate, -pollster) %&gt;% \n  mutate(candidate = factor(candidate, levels = c(\"Trump\",\"Clinton\")))%&gt;%\n  group_by(pollster) %&gt;%\n  filter(n()&gt;=10) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(enddate, percentage, color = candidate)) +  \n  geom_point(show.legend = FALSE, alpha=0.4)  + \n  geom_smooth(method = \"loess\", span = 0.15) +\n  scale_y_continuous(limits = c(30,50))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 22 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\nStatistical Models\n\n\nPredicting Electoral College Votes\n\nTop 5 States w/ Most Electoral Votes\nRemoved my initial models in favor of this comprehensive example I found online - more insightful tutorial than my basic regressions\n\nresults_us_election_2016 %&gt;% top_n(5, electoral_votes)\n\n         state electoral_votes clinton trump others\n1   California              55    61.7  31.6    6.7\n2        Texas              38    43.2  52.2    4.5\n3      Florida              29    47.8  49.0    3.2\n4     New York              29    59.0  36.5    4.5\n5     Illinois              20    55.8  38.8    5.4\n6 Pennsylvania              20    47.9  48.6    3.6\n\n\n\n\n\nAggregrate Poll Results - Week Before Election\n\nresults &lt;- polls_us_election_2016 %&gt;%\n  filter(state!=\"U.S.\" & \n           !grepl(\"CD\", state) & \n           enddate &gt;=\"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) %&gt;%\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;%\n  group_by(state) %&gt;%\n  summarize(avg = mean(spread), sd = sd(spread), n = n()) %&gt;%\n  mutate(state = as.character(state))\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 4\n$ state &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Con…\n$ avg   &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000, 0.0452000…\n$ sd    &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317, 0.029459…\n$ n     &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, 3, 4, 6, 3…\n\n\n\n\nJoin the number of electoral votes for each state\n\nresults &lt;- left_join(results, results_us_election_2016, by = \"state\")\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 8\n$ state           &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Color…\n$ avg             &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000,…\n$ sd              &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317…\n$ n               &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, …\n$ electoral_votes &lt;int&gt; 9, 11, 6, 55, 9, 7, 3, 29, 16, 4, 4, 20, 11, 6, 6, 8, …\n$ clinton         &lt;dbl&gt; 34.4, 45.1, 33.7, 61.7, 48.2, 54.6, 53.4, 47.8, 45.9, …\n$ trump           &lt;dbl&gt; 62.1, 48.7, 60.6, 31.6, 43.3, 40.9, 41.9, 49.0, 51.0, …\n$ others          &lt;dbl&gt; 3.6, 6.2, 5.8, 6.7, 8.6, 4.5, 4.7, 3.2, 3.1, 7.7, 13.2…\n\n\n\n\nStates with no poll data - high confidence of winner\n\nresults_us_election_2016 %&gt;% filter(!state %in% results$state) %&gt;% \n  pull(state)\n\n[1] \"Rhode Island\"         \"Alaska\"               \"Wyoming\"             \n[4] \"District of Columbia\"\n\n\n\n\nEstimate standard deviation\n\nresults &lt;- results %&gt;%\n  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 8\n$ state           &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Color…\n$ avg             &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000,…\n$ sd              &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317…\n$ n               &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, …\n$ electoral_votes &lt;int&gt; 9, 11, 6, 55, 9, 7, 3, 29, 16, 4, 4, 20, 11, 6, 6, 8, …\n$ clinton         &lt;dbl&gt; 34.4, 45.1, 33.7, 61.7, 48.2, 54.6, 53.4, 47.8, 45.9, …\n$ trump           &lt;dbl&gt; 62.1, 48.7, 60.6, 31.6, 43.3, 40.9, 41.9, 49.0, 51.0, …\n$ others          &lt;dbl&gt; 3.6, 6.2, 5.8, 6.7, 8.6, 4.5, 4.7, 3.2, 3.1, 7.7, 13.2…\n\n\n\n\nSimulate election day voting with Monte Carlo\n\nmu &lt;- 0\ntau &lt;- 0.02\nresults &lt;- results %&gt;% mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1/ (1/sigma^2 + 1/tau^2)))\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 12\n$ state           &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Color…\n$ avg             &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000,…\n$ sd              &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317…\n$ n               &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, …\n$ electoral_votes &lt;int&gt; 9, 11, 6, 55, 9, 7, 3, 29, 16, 4, 4, 20, 11, 6, 6, 8, …\n$ clinton         &lt;dbl&gt; 34.4, 45.1, 33.7, 61.7, 48.2, 54.6, 53.4, 47.8, 45.9, …\n$ trump           &lt;dbl&gt; 62.1, 48.7, 60.6, 31.6, 43.3, 40.9, 41.9, 49.0, 51.0, …\n$ others          &lt;dbl&gt; 3.6, 6.2, 5.8, 6.7, 8.6, 4.5, 4.7, 3.2, 3.1, 7.7, 13.2…\n$ sigma           &lt;dbl&gt; 0.014623079, 0.008984912, 0.000700000, 0.017321894, 0.…\n$ B               &lt;dbl&gt; 0.348358497, 0.167929753, 0.001223501, 0.428610610, 0.…\n$ posterior_mean  &lt;dbl&gt; -0.097376962, -0.027162471, -0.151214762, 0.148618380,…\n$ posterior_se    &lt;dbl&gt; 0.0118043805, 0.0081958466, 0.0006995716, 0.0130936719…\n\n\n\n\nSimulate 10,000 times\n\nB &lt;- 10000\nmu &lt;- 0\ntau &lt;- 0.02\nclinton_EV = replicate(B, {\n  results %&gt;% mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1 / (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result &gt; 0, electoral_votes, 0)) %&gt;%\n    summarize(clinton = sum(clinton)) %&gt;%\n    pull(clinton) + 7\n})\nmean(clinton_EV &gt; 269)\n\n[1] 0.9984\n\n\n\n\nInclude general bias\n\ntau &lt;- 0.02\nbias_sd &lt;- 0.03\nclinton_EV_2 &lt;- replicate(1000, {\n  results %&gt;% mutate(sigma = sqrt(sd^2/n  + bias_sd^2),  \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B*mu + (1-B)*avg,\n                   posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result&gt;0, electoral_votes, 0)) %&gt;% \n    summarize(clinton = sum(clinton) + 7) %&gt;% \n    pull(clinton)\n})\nmean(clinton_EV_2 &gt; 269)\n\n[1] 0.823\n\n\n\nhist(clinton_EV)\n\n\n\nhist(clinton_EV_2)"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#modeling-results",
    "href": "coding-exercise/coding-exercise.html#modeling-results",
    "title": "R Coding Exercise",
    "section": "Modeling Results",
    "text": "Modeling Results\nTwo models were ran to predict the amount of electoral votes Hillary Clinton would receive in the 2016 election. The first model concluded that Clinton had a 99% chance of winning the presidency with more than 269 electoral votes. However, the first model assumes results from different states are independent, ignoring the general bias. Model 2 addresses these flaws, calculating an 82.5% chance of Clinton winning the presidency. The histograms illustrate Model 2’s robustness compared to the first model, showing how including bias adds more variability in the final predictions."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "aboutme.html#cool-data-analysis-resources",
    "href": "aboutme.html#cool-data-analysis-resources",
    "title": "About Me",
    "section": "Cool Data Analysis Resources",
    "text": "Cool Data Analysis Resources\nI recently came across this fascinating video on Youtube: What does data and analytics have to do with shoes? that explains how data analytics is used in a major sporting brand, such as Nike. It’s an excellent example of how data analysis can help inspire new ideas and trends for all professional athletes around the world! In addition, I found this Harvard Article: Nike: It’s Data Analytics, Just Do It that highlights the digital innovation and transformation used in the Nike brand."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Howdy y’all!\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nFeel free to navigate around my website on the various sections shown above:\n\nAbout Me: Learn more about my background, education, and interests.\nProjects: Check out some of the exciting data analysis projects I’ve worked on during my DA 6833 Practicum II course.\n\nConnect with me on LinkedIn: Stay in touch and follow my professional journey!\n\nThank you for visiting, and I hope you find my work both interesting and informative!"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exploration on Weekly United States Hospitalization Metrics by Jurisdiction",
    "section": "",
    "text": "Title: Weekly United States Hospitalization Metrics by Jurisdiction, During Mandatory Reporting Period from August 1, 2020 to April 30, 2024, and for Data Reported Voluntarily Beginning May 1, 2024, National Healthcare Safety Network (NHSN)\nSource: (https://data.cdc.gov/Public-Health-Surveillance/Weekly-United-States-Hospitalization-Metrics-by-Ju/aemt-mg7g/about_data)\nDescription: This dataset provides information on COVID-19 and influenza-related hospitalizations, hospital occupancy, and hospital capacity across different jurisdictions in the United States from August 1, 2020, to April 30, 2024, and data reported voluntarily from May 1, 2024. It includes both continuous and categorical variables. I will further explore the dataset from the CDC data website. Specifically looking at information on hospitalization metrics, including the number of new admissions, total number of beds occupied, and more."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#explanation-and-comparison",
    "href": "cdcdata-exercise/cdcdata-exercise.html#explanation-and-comparison",
    "title": "CDC Data Exploration on Weekly United States Hospitalization Metrics by Jurisdiction",
    "section": "Explanation and Comparison:",
    "text": "Explanation and Comparison:\nGenerating Synthetic Data:\nThe synthetic data is generated with the same structure and similar statistical properties as the original data. This includes date ranges, geographic aggregation categories, and numeric ranges for key metrics.\nThe faux package is used to create the synthetic data with the required number of observations and similar distribution characteristics."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#similarities",
    "href": "cdcdata-exercise/cdcdata-exercise.html#similarities",
    "title": "CDC Data Exploration on Weekly United States Hospitalization Metrics by Jurisdiction",
    "section": "Similarities:",
    "text": "Similarities:\nThe EDA on synthetic data involves summarizing key variables and plotting distributions and boxplots to understand central tendencies and variability.\nThe synthetic data plot closely mirrors the distribution characteristics of the original data plot, particularly in terms of the right-skewed distribution and concentration of lower values for Weekly Average COVID-19 Admissions .\nThe histograms show the distribution of weekly averages for COVID-19 admissions, inpatient beds occupied, and ICU beds occupied. They exhibit similar patterns to the original data, with right-skewed distributions.\nThe boxplots provide insights into variability across different states, with central tendencies and outliers indicating similarities and differences in hospital metrics."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Introduction\nIn this analysis, I will reproduce a graph from FiveThirtyEight. The dataset titled fight-songs.csv includes information about fight songs from schools in the Power Five collegiate conferences — the ACC, Big Ten, Big 12, Pac-12, and SEC — as well as Notre Dame. Some schools feature multiple fight songs, and in some cases, the songs officially endorsed by the schools are not the ones most popular among fans. The songs selected were the ones that are most recognized and cherished. For the analysis, the data was focused on the most commonly sung lyrics that are also published by the schools, excluding certain verses. The tempo and length data are based on the versions of the songs found on Spotify. I will be creating a graph on How Texas A&M’s fight song stacks up because Texas A&M is my undergraduate alma mater and I used to sing the fight song all the time at football games with my fellow Aggies. With this in mind, I will be using ChatGPT 4o to recreate the graph.\n\n\nData\nThe data used for this analysis is from the FiveThirtyEight College Fight Song Lyrics\nhttps://projects.fivethirtyeight.com/college-fight-song-lyrics/\n\n\nChatGPT 4o Prompts Asked to Reproduce Graphs/Tables/Code\n\nInitial Graph Recreation\n\nCan you recreate this graph for Texas A&M?\nCan you get rid of this saying hex to the side of the graph?\nCan you add where I can hover over the other schools’ points and see their names too?\n\nError Fixing and Debugging\n\nError in geom_point(data = others, size = 5, alpha = 0.2, color = others$hex)\nError in geom_point(aes(color = hex), size = 5, alpha = 0.1)\nError in geom_point(data = others, aes(color = hex), size = 5, alpha = 0.2)\nError in UseMethod(“ggplotly”, p)\n\nModifications to Graph\n\nCan you make the other schools’ points be random colors?\n\nTable Creation\n\nCreate a table that looks exactly like this:\nAdd a separate table showing this:\nCan you make it match the bell emojis?\n\nPublication Quality Table\n\nCreate a publication quality table with the dataset I provide\n\n\nThese questions guided the step-by-step creation and modification of my graphs and tables in R, as well as the inclusion of your prompts and questions in my Quarto document.\n\n\nLoad necessary libraries\n\nlibrary(readr) \nlibrary(dplyr) \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2) \nlibrary(gt)\nlibrary(ggrepel)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(tibble)\nlibrary(emoji)\n\n\nAttaching package: 'emoji'\n\nThe following object is masked from 'package:ggplot2':\n\n    arrow\n\n\n\n\nLoad the data\n\nfight_songs &lt;- read_csv(\"/Users/sethharris/Desktop/DA-6833-02T Practicum II/sethharris-P2-portfolio/presentation-exercise/fight-songs.csv\")\n\nRows: 65 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (19): school, conference, song_name, writers, year, student_writer, offi...\ndbl  (4): bpm, sec_duration, number_fights, trope_count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfight_songs$trope_count&lt;- as.factor(fight_songs$trope_count)\n\n\n\nExplore the data\n\nhead(fight_songs)\n\n# A tibble: 6 × 23\n  school conference song_name writers year  student_writer official_song contest\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;  \n1 Notre… Independe… Victory … \"Micha… 1908  No             Yes           No     \n2 Baylor Big 12     Old Fight \"Dick … 1947  Yes            Yes           No     \n3 Iowa … Big 12     Iowa Sta… \"Jack … 1930  Yes            Yes           No     \n4 Kansas Big 12     I'm a Ja… \"Georg… 1912  Yes            Yes           No     \n5 Kansa… Big 12     Wildcat … \"Harry… 1927  Yes            Yes           No     \n6 Oklah… Big 12     Boomer S… \"Arthu… 1905  Yes            Yes           No     \n# ℹ 15 more variables: bpm &lt;dbl&gt;, sec_duration &lt;dbl&gt;, fight &lt;chr&gt;,\n#   number_fights &lt;dbl&gt;, victory &lt;chr&gt;, win_won &lt;chr&gt;, victory_win_won &lt;chr&gt;,\n#   rah &lt;chr&gt;, nonsense &lt;chr&gt;, colors &lt;chr&gt;, men &lt;chr&gt;, opponents &lt;chr&gt;,\n#   spelling &lt;chr&gt;, trope_count &lt;fct&gt;, spotify_id &lt;chr&gt;\n\nstr(fight_songs)\n\nspc_tbl_ [65 × 23] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ school         : chr [1:65] \"Notre Dame\" \"Baylor\" \"Iowa State\" \"Kansas\" ...\n $ conference     : chr [1:65] \"Independent\" \"Big 12\" \"Big 12\" \"Big 12\" ...\n $ song_name      : chr [1:65] \"Victory March\" \"Old Fight\" \"Iowa State Fights\" \"I'm a Jayhawk\" ...\n $ writers        : chr [1:65] \"Michael J. Shea and John F. Shea\" \"Dick Baker and Frank Boggs\" \"Jack Barker, Manly Rice, Paul Gnam, Rosalind K. Cook\" \"George \\\"Dumpy\\\" Bowles\" ...\n $ year           : chr [1:65] \"1908\" \"1947\" \"1930\" \"1912\" ...\n $ student_writer : chr [1:65] \"No\" \"Yes\" \"Yes\" \"Yes\" ...\n $ official_song  : chr [1:65] \"Yes\" \"Yes\" \"Yes\" \"Yes\" ...\n $ contest        : chr [1:65] \"No\" \"No\" \"No\" \"No\" ...\n $ bpm            : num [1:65] 152 76 155 137 80 153 180 81 149 159 ...\n $ sec_duration   : num [1:65] 64 99 55 62 67 37 29 65 47 54 ...\n $ fight          : chr [1:65] \"Yes\" \"Yes\" \"Yes\" \"No\" ...\n $ number_fights  : num [1:65] 1 4 5 0 6 0 5 17 2 8 ...\n $ victory        : chr [1:65] \"Yes\" \"Yes\" \"No\" \"No\" ...\n $ win_won        : chr [1:65] \"Yes\" \"Yes\" \"No\" \"No\" ...\n $ victory_win_won: chr [1:65] \"Yes\" \"Yes\" \"No\" \"No\" ...\n $ rah            : chr [1:65] \"Yes\" \"No\" \"Yes\" \"No\" ...\n $ nonsense       : chr [1:65] \"No\" \"No\" \"No\" \"Yes\" ...\n $ colors         : chr [1:65] \"Yes\" \"Yes\" \"No\" \"No\" ...\n $ men            : chr [1:65] \"Yes\" \"No\" \"Yes\" \"Yes\" ...\n $ opponents      : chr [1:65] \"No\" \"No\" \"No\" \"Yes\" ...\n $ spelling       : chr [1:65] \"No\" \"Yes\" \"Yes\" \"No\" ...\n $ trope_count    : Factor w/ 8 levels \"0\",\"1\",\"2\",\"3\",..: 7 6 5 4 4 3 5 5 7 4 ...\n $ spotify_id     : chr [1:65] \"15a3ShKX3XWKzq0lSS48yr\" \"2ZsaI0Cu4nz8DHfBkPt0Dl\" \"3yyfoOXZQCtR6pfRJqu9pl\" \"0JzbjZgcjugS0dmPjF9R89\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   school = col_character(),\n  ..   conference = col_character(),\n  ..   song_name = col_character(),\n  ..   writers = col_character(),\n  ..   year = col_character(),\n  ..   student_writer = col_character(),\n  ..   official_song = col_character(),\n  ..   contest = col_character(),\n  ..   bpm = col_double(),\n  ..   sec_duration = col_double(),\n  ..   fight = col_character(),\n  ..   number_fights = col_double(),\n  ..   victory = col_character(),\n  ..   win_won = col_character(),\n  ..   victory_win_won = col_character(),\n  ..   rah = col_character(),\n  ..   nonsense = col_character(),\n  ..   colors = col_character(),\n  ..   men = col_character(),\n  ..   opponents = col_character(),\n  ..   spelling = col_character(),\n  ..   trope_count = col_double(),\n  ..   spotify_id = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ncolnames(fight_songs)\n\n [1] \"school\"          \"conference\"      \"song_name\"       \"writers\"        \n [5] \"year\"            \"student_writer\"  \"official_song\"   \"contest\"        \n [9] \"bpm\"             \"sec_duration\"    \"fight\"           \"number_fights\"  \n[13] \"victory\"         \"win_won\"         \"victory_win_won\" \"rah\"            \n[17] \"nonsense\"        \"colors\"          \"men\"             \"opponents\"      \n[21] \"spelling\"        \"trope_count\"     \"spotify_id\"     \n\nsummary(fight_songs)\n\n    school           conference         song_name           writers         \n Length:65          Length:65          Length:65          Length:65         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     year           student_writer     official_song        contest         \n Length:65          Length:65          Length:65          Length:65         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n      bpm         sec_duration       fight           number_fights   \n Min.   : 65.0   Min.   : 27.00   Length:65          Min.   : 0.000  \n 1st Qu.: 90.0   1st Qu.: 58.00   Class :character   1st Qu.: 0.000  \n Median :140.0   Median : 67.00   Mode  :character   Median : 2.000  \n Mean   :128.8   Mean   : 71.91                      Mean   : 2.846  \n 3rd Qu.:151.0   3rd Qu.: 85.00                      3rd Qu.: 5.000  \n Max.   :180.0   Max.   :172.00                      Max.   :17.000  \n                                                                     \n   victory            win_won          victory_win_won        rah           \n Length:65          Length:65          Length:65          Length:65         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   nonsense            colors              men             opponents        \n Length:65          Length:65          Length:65          Length:65         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   spelling          trope_count  spotify_id       \n Length:65          3      :17   Length:65         \n Class :character   4      :14   Class :character  \n Mode  :character   5      :14   Mode  :character  \n                    1      : 6                     \n                    6      : 6                     \n                    2      : 4                     \n                    (Other): 4                     \n\n\n\n\nClean the data\n\n# Add a hex column with color codes since it is missing\nset.seed(123)  # For reproducibility of random colors\nfight_songs &lt;- fight_songs %&gt;%\n  mutate(hex = case_when(\n    school == \"Texas A&M\" ~ \"#500000\",  # Example color for Texas A&M\n    school == \"Oregon State\" ~ \"orange\",  # Example color for Oregon State\n    TRUE ~ sample(colors(), n(), replace = TRUE)  # Random colors for other schools\n  ))\n\n# Select \"Texas A&M\" and create a separate dataset\ntamu &lt;- fight_songs %&gt;% filter(school == \"Texas A&M\")\n\n# Create a dataset for the schools that are not \"Texas A&M\"\nothers &lt;- fight_songs %&gt;% filter(school != \"Texas A&M\")\n\n\n\nRecreate the graph\n\n# Plot the data\np &lt;- fight_songs %&gt;% \n  ggplot(aes(x = sec_duration, y = bpm, text = school)) +\n  geom_point(data = others, aes(color = hex), size = 5, alpha = 0.7) +\n  geom_point(data = tamu, size = 5, shape = 21, fill = tamu$hex, color = \"black\", stroke = 1.2) +\n  geom_hline(yintercept = mean(fight_songs$bpm), linetype = \"dashed\") +\n  geom_vline(xintercept = mean(fight_songs$sec_duration), linetype = \"dashed\") +\n  ggtitle(\"How Texas A&M’s fight song stacks up\") +\n  annotate(\"text\", x = 160, y = 144, label = \"Texas A&M\", size = 4, fontface = 'bold') +\n  annotate(\"text\", x = c(30, 140), y = 190, label = c(\"Fast and short\", \"Fast but long\")) +\n  annotate(\"text\", x = c(30, 140), y = 50, label = c(\"Slow but short\", \"Slow and long\")) +\n  annotate(\"text\", x = 160, y = 131, label = \"Average\", size = 3) +\n  annotate(\"text\", x = 70, y = 105, label = \"Average\", size = 3, angle = 90) +\n  scale_x_discrete(limit = c(0, 20, 40, 60, 80, 100, 120, 140, 160, 180), name = \"Duration\", \n                   labels = c(\"0 sec\", \"20\", \"40\", \"60\", \"80\", \"100\", \"120\", \"140\", \"160\", \"180\")) +\n  scale_y_discrete(limit = c(40, 60, 80, 100, 120, 140, 160, 180, 200), name = \"Beats per minute\", \n                   labels = c(\"\", \"60\", \"80\", \"100\", \"120\", \"140\", \"160\", \"180\", \"200bpm\")) +\n  coord_fixed(ratio = 1, xlim = c(0, 180), ylim = c(40, 200)) +\n  theme(axis.ticks = element_blank(), panel.background = element_rect(fill = \"white\"),\n        panel.grid.major = element_line(colour = \"lightgrey\"), \n        plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        axis.title.x = element_text(face = \"bold\"),\n        axis.title.y = element_text(face = \"bold\"),\n        legend.position = \"none\")\n\nWarning in scale_x_discrete(limit = c(0, 20, 40, 60, 80, 100, 120, 140, : Continuous limits supplied to discrete scale.\nℹ Did you mean `limits = factor(...)` or `scale_*_continuous()`?\n\n\nWarning in scale_y_discrete(limit = c(40, 60, 80, 100, 120, 140, 160, 180, : Continuous limits supplied to discrete scale.\nℹ Did you mean `limits = factor(...)` or `scale_*_continuous()`?\n\n# Convert the plot to plotly\np_plotly &lt;- ggplotly(p, tooltip = \"text\")\n\n# Display the plot\np_plotly\n\n\n\n\n\n\n\nCreate a table documenting the Texas A&M 4 fight song clichés\n\n# Create a dataframe with the clichés and their presence\ncliches &lt;- data.frame(\n  Cliche = c(\"Fight\", \"Victory\", \"Win\", \"Rah\", \"Nonsense syllables\", \"School colors\", \"Men/“boys”/“sons”\", \"Name of opponent\", \"Spell something out\"),\n  Present = c(TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE)\n)\n\n# Generate the table using gt\ncliches %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = md(\"**Texas A&M Fight Song Clichés**\")\n  ) %&gt;%\n  cols_label(\n    Cliche = \"\",\n    Present = \"\"\n  ) %&gt;%\n  text_transform(\n    locations = cells_body(columns = vars(Cliche)),\n    fn = function(x) {\n      case_when(\n        x == \"Fight\" ~ paste0(emoji(\"mega\"), \" “Fight”\"),\n        x == \"Victory\" ~ \"❌ “Victory”\",\n        x == \"Win\" ~ \"❌ “Win”\",\n        x == \"Rah\" ~ \"❌ “Rah”\",\n        x == \"Nonsense syllables\" ~ paste0(emoji(\"mega\"), \" Nonsense syllables\"),\n        x == \"School colors\" ~ \"❌ School colors\",\n        x == \"Men/“boys”/“sons”\" ~ paste0(emoji(\"mega\"), \" “Men”/“boys”/“sons”\"),\n        x == \"Name of opponent\" ~ paste0(emoji(\"mega\"), \" Name of opponent\"),\n        x == \"Spell something out\" ~ \"❌ Spell something out\"\n      )\n    }\n  ) %&gt;%\n  cols_hide(columns = vars(Present)) %&gt;%\n  tab_footnote(\n    footnote = md(\"**4 fight song clichés**\"),\n    locations = cells_title(groups = \"title\")\n  ) %&gt;%\n  tab_options(\n    column_labels.hidden = TRUE,\n    table.width = pct(100),\n    table.font.size = \"large\",\n    data_row.padding = px(10)\n  )\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\nSince gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n  \n    \n      Texas A&M Fight Song Clichés1\n    \n    \n  \n  \n    📣 “Fight”\n    ❌ “Victory”\n    ❌ “Win”\n    ❌ “Rah”\n    📣 Nonsense syllables\n    ❌ School colors\n    📣 “Men”/“boys”/“sons”\n    📣 Name of opponent\n    ❌ Spell something out\n  \n  \n  \n    \n      1 4 fight song clichés\n    \n  \n\n\n\n\n\n\nCreate a graph showing if student-writers or non-student writers wrote longer college fight songs\n\n# Create the density plot\nggplot(fight_songs) +\n  aes(x = sec_duration, fill = student_writer) +\n  geom_density(adjust = 1L, alpha=0.5) +\n  annotate(\"segment\", x = 50, xend = 50, y = 0, yend = 0.02, colour = \"blue\", linetype=\"dotted\") +\n  annotate(\"segment\", x = 100, xend = 100, y = 0, yend = 0.02, colour = \"red\", linetype=\"dashed\") +\n  scale_fill_manual(values = c(No = \"blue\", Unknown = \"red\", Yes = \"white\")) +\n  labs(x = \"Duration (in seconds)\", y = \"Density\",\n       title = \"Who Wrote Longer College Fight Songs? \\nStudent Writers or Not-Student Writers?\",\n       subtitle = \"Student writers write longer fight songs. Some songs are longer than 150 seconds.\",\n       caption = \"The songs written by student writers are between 50 to 125 seconds. Some songs go beyond 150.\\nThe songs by not-student writers mostly fall between 50 and 100 seconds and stay below 150.\\nThe songs written by unknown writers are less than 100 seconds.\") +\n  theme_minimal() +\n  theme(legend.position = \"none\", plot.caption = element_text(hjust = 0),\n        panel.grid.minor = element_blank(), panel.grid.major = element_blank()) +\n  facet_grid(vars(), vars(student_writer))\n\n\n\n\nThis graph examines the duration of college fight songs written by student writers compared to those written by non-student writers and those with unknown authorship. The plot is divided into three density plots, each representing a different category of writers: No, Unknown, and Yes.\n\nKey Observations:\n\nNon-Student Writers (No):\n\nThe distribution of fight song durations for non-student writers shows a peak around 50-60 seconds.\nThe density decreases after 60 seconds, with a smaller peak around 100 seconds.\nThe blue dotted line at 50 seconds and the red dashed line at 100 seconds highlight significant points in the distribution.\n\nUnknown Writers (Unknown):\n\nFight songs with unknown authorship have a peak at around 50-60 seconds, similar to non-student writers.\nThe density drops sharply after 60 seconds, and there are very few songs longer than 100 seconds.\nThe blue dotted line at 50 seconds and the red dashed line at 100 seconds are also significant markers for this group.\n\nStudent Writers (Yes):\n\nSongs written by student writers exhibit a peak at around 50-60 seconds.\nUnlike the other two categories, student-written songs show a wider distribution of durations, with some songs extending up to 150 seconds.\nThe blue dotted line at 50 seconds and the red dashed line at 100 seconds indicate key points in this distribution as well.\n\n\n\n\nSummary:\n\nStudent writers tend to write longer fight songs compared to non-student writers and those with unknown authorship.\nMost fight songs, regardless of the writer category, tend to peak around the 50-60 seconds mark.\nThere is a notable difference in the maximum duration of songs, with student-written songs having a broader range and extending up to 150 seconds, whereas non-student and unknown writers’ songs rarely exceed 100 seconds.\n\nThe annotations and color coding help to emphasize the important points and differences in the duration distributions across the three categories of writers.\n\n\n\nCreate a publication quality table\n\n# Select key columns for the table\ncliches_data &lt;- fight_songs %&gt;%\n  select(school, conference, song_name, writers, year, bpm, sec_duration, fight, victory, win_won, rah, nonsense, colors, men, opponents, spelling, trope_count)\n\n# Filter to include a subset of schools for demonstration\ncliches_data &lt;- cliches_data %&gt;%\n  filter(school %in% c(\"Texas A&M\", \"Oregon State\", \"Alabama\", \"Florida\", \"LSU\"))\n\n# Rename columns for better readability\ncolnames(cliches_data) &lt;- c(\"School\", \"Conference\", \"Song Name\", \"Writers\", \"Year\", \"BPM\", \"Duration (sec)\", \"Fight\", \"Victory\", \"Win\", \"Rah\", \"Nonsense Syllables\", \"School Colors\", \"Men/Boys/Sons\", \"Name of Opponent\", \"Spell Something Out\", \"Trope Count\")\n\n# Replace boolean values with symbols\ncliches_data &lt;- cliches_data %&gt;%\n  mutate(\n    Fight = ifelse(Fight == \"Yes\", \"✔\", \"✘\"),\n    Victory = ifelse(Victory == \"Yes\", \"✔\", \"✘\"),\n    Win = ifelse(Win == \"Yes\", \"✔\", \"✘\"),\n    Rah = ifelse(Rah == \"Yes\", \"✔\", \"✘\"),\n    `Nonsense Syllables` = ifelse(`Nonsense Syllables` == \"Yes\", \"✔\", \"✘\"),\n    `School Colors` = ifelse(`School Colors` == \"Yes\", \"✔\", \"✘\"),\n    `Men/Boys/Sons` = ifelse(`Men/Boys/Sons` == \"Yes\", \"✔\", \"✘\"),\n    `Name of Opponent` = ifelse(`Name of Opponent` == \"Yes\", \"✔\", \"✘\"),\n    `Spell Something Out` = ifelse(`Spell Something Out` == \"Yes\", \"✔\", \"✘\")\n  )\n\n# Create the table\ncliches_table &lt;- cliches_data %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = md(\"**Fight Song Attributes**\"),\n    subtitle = \"Presence of Various Clichés in Fight Songs\"\n  ) %&gt;%\n  cols_label(\n    School = \"School\",\n    Conference = \"Conference\",\n    `Song Name` = \"Song Name\",\n    Writers = \"Writers\",\n    Year = \"Year\",\n    BPM = \"BPM\",\n    `Duration (sec)` = \"Duration (sec)\",\n    Fight = \"Fight\",\n    Victory = \"Victory\",\n    Win = \"Win\",\n    Rah = \"Rah\",\n    `Nonsense Syllables` = \"Nonsense Syllables\",\n    `School Colors` = \"School Colors\",\n    `Men/Boys/Sons` = \"Men/Boys/Sons\",\n    `Name of Opponent` = \"Name of Opponent\",\n    `Spell Something Out` = \"Spell Something Out\",\n    `Trope Count` = \"Trope Count\"\n  ) %&gt;%\n  fmt_markdown(\n    columns = vars(Fight, Victory, Win, Rah, `Nonsense Syllables`, `School Colors`, `Men/Boys/Sons`, `Name of Opponent`, `Spell Something Out`)\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_title(groups = \"title\")\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.align = \"center\"\n  )\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n# Print the table\ncliches_table\n\n\n\n\n\n  \n    \n      Fight Song Attributes\n    \n    \n      Presence of Various Clichés in Fight Songs\n    \n    \n      School\n      Conference\n      Song Name\n      Writers\n      Year\n      BPM\n      Duration (sec)\n      Fight\n      Victory\n      Win\n      Rah\n      Nonsense Syllables\n      School Colors\n      Men/Boys/Sons\n      Name of Opponent\n      Spell Something Out\n      Trope Count\n    \n  \n  \n    Oregon State\nPac-12\nHail to Old OSU\nHarold Wilkins\n1914\n137\n63\n✔\n\n✘\n\n✘\n\n✘\n\n✘\n\n✘\n\n✘\n\n✘\n\n✔\n\n2\n    Alabama\nSEC\nYea Alabama\nEthelred Lundy (Epp) Sykes\n1926\n153\n59\n✔\n\n✔\n\n✔\n\n✘\n\n✘\n\n✔\n\n✔\n\n✔\n\n✘\n\n6\n    Florida\nSEC\nThe Orange and Blue\nGeorge Hamilton and Thornton Whitney Allen\n1925\n71\n66\n✔\n\n✘\n\n✘\n\n✘\n\n✘\n\n✔\n\n✘\n\n✘\n\n✘\n\n2\n    LSU\nSEC\nFight for LSU\nCastro Carazo and W.G. Higginbotham\n1937\n168\n83\n✔\n\n✘\n\n✔\n\n✔\n\n✘\n\n✔\n\n✘\n\n✘\n\n✔\n\n5\n    Texas A&M\nSEC\nAggie War Hymn\nJ.V. “Pinky” Wilson\n1918\n117\n172\n✔\n\n✘\n\n✘\n\n✘\n\n✔\n\n✘\n\n✔\n\n✔\n\n✘\n\n4\n  \n  \n  \n\n\n\n\nThis table effectively communicates the presence of common clichés in the fight songs of selected schools, with clear and professional formatting. By leveraging the gt package, I ensured the table is visually appealing and easy to understand.\n\n\nConclusion\nIn the graph titled “How Texas A&M’s fight song stacks up,” Texas A&M’s fight song stands out for its relatively long duration and moderately fast tempo. Plotted towards the bottom right quadrant, the song is categorized as “Slow and long” compared to others. The average beat per minute (bpm) of the fight songs is marked at approximately 117 bpm, and the average duration is around 172 seconds. Texas A&M’s song, with a higher duration and a slightly lower, but moderate bpm than the average, shows a unique position among the other schools’ fight songs.\nAmong the songs reviewed, 44 incorporate the term “fight,” with it appearing a total of 185 times. Forty-one songs include either “win” or “victory,” while 35 reference the school’s colors. Twenty-nine songs feature some form of spelling, including acronyms, 18 use the word “rah,” and 10 contain other nonsensical syllables. Additionally, 24 songs mention men in terms like “boys” or “sons,” and 12 name the school’s opponent.\nThe “Aggie War Hymn” by Texas A&M, written in 1918 by J.V. “Pinky” Wilson, includes four of these clichés. Notably, the song mentions the colors orange and white, which are those of the University of Texas, rather than Texas A&M’s own colors."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "title: “Manuscript/Report Template for a Data Analysis Project” subtitle: “” author: Seth Harris date: today format: html: toc: false number-sections: true highlight-style: github bibliography: ../dataanalysis-template-references.bib csl: ../apa.csl"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#joaquin-ramirez-contributed-to-this-exercise",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#joaquin-ramirez-contributed-to-this-exercise",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "0.1 JOAQUIN RAMIREZ CONTRIBUTED TO THIS EXERCISE",
    "text": "0.1 JOAQUIN RAMIREZ CONTRIBUTED TO THIS EXERCISE\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. The exampledata.xlsx is the original dataset given. The dataset is composed of several variables, such as Height, Weight and Gender columns of a sample population. I added two more columns to the original dataset, one being numerical and the other being categorical. This data demonstrates data analysis that is used in R. My two added columns were WorkHoursPerWeek and EducationLevel to demonstrate the number of hours worked per week (numerical) and highest level of education attained (categorical) with the descriptions of my new variables being added in the Codebook sheet. I then saved it as exampledata2.xlsx."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n8\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nEducationLevel\n0\n1\n3\n9\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(here)\n\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n# Create a boxplot of Height by Education Level\np_boxplot &lt;- ggplot(mydata, aes(x = EducationLevel, y = Height)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot of Height by Education Level\",\n    x = \"Education Level\",\n    y = \"Height\"\n  )\n\n# Display the boxplot\nprint(p_boxplot)\n\n\n\n# Save the boxplot to a file\nggsave(\"boxplot.png\", plot = p_boxplot)\n\nSaving 7 x 5 in image\n\n# Create a scatterplot with Weight on the x-axis and Work Hours Per Week on the y-axis\np_scatterplot &lt;- ggplot(mydata, aes(x = Weight, y = WorkHoursPerWeek)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Weight and Work Hours Per Week\",\n    x = \"Weight\",\n    y = \"Work Hours Per Week\"\n  )\n\n# Display the scatterplot\nprint(p_scatterplot)\n\n\n\n# Save the scatterplot to a file\nggsave(\"scatterplot.png\", plot = p_scatterplot)\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\n\n\nTable 3: Another linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n183\n9.899495\n18.4857916\n0.0344048\n\n\nWorkHoursPerWeek15\n-50\n14.000000\n-3.5714286\n0.1738027\n\n\nWorkHoursPerWeek30\n-27\n14.000000\n-1.9285714\n0.3045286\n\n\nWorkHoursPerWeek35\n-8\n14.000000\n-0.5714286\n0.6695013\n\n\nWorkHoursPerWeek37\n-29\n14.000000\n-2.0714286\n0.2863259\n\n\nWorkHoursPerWeek38\n-17\n14.000000\n-1.2142857\n0.4385829\n\n\nWorkHoursPerWeek40\n-10\n12.124356\n-0.8247861\n0.5609407\n\n\nWorkHoursPerWeek45\n-5\n14.000000\n-0.3571429\n0.7816242\n\n\nEducationLevelBachelor\nNA\nNA\nNA\nNA\n\n\nEducationLevelMaster\nNA\nNA\nNA\nNA\n\n\nEducationLevelNone\nNA\nNA\nNA\nNA\n\n\nEducationLevelPhD\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nThe boxplot shows that individuals with a Bachelor’s degree tend to have the highest median height, followed by those with a Master’s degree. Individuals with an Associate’s degree have the lowest median height. There are also notable outliers among those with an Associate’s degree. The categories “None” and “PhD” have very limited data, showing a narrower range of heights.\nThe scatterplot shows no linear relationship between the variables. Data points are scattered widely across the plot, indicating that weight does not consistently predict the number of work hours per week for the individuals in the dataset.\nresulttable3.rds - output: The linear model fit table shows that the intercept is significant with a p-value of 0.034. However, none of the work hours per week predictors are statistically significant, with p-values ranging from 0.17 to 0.78, and the education level predictors were not included in the model due to missing data."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Conclusions",
    "text": "6.1 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/sethharris/Desktop/DA-6833-02T Practicum II/sethharris-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "href": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/sethharris/Desktop/DA-6833-02T Practicum II/sethharris-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                2     \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate min max empty n_unique whitespace\n1 WorkHoursPerWeek         0             1   1   2     0        8          0\n2 EducationLevel           0             1   3   9     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a boxplot of Height by Education Level\np_boxplot &lt;- ggplot(mydata, aes(x = EducationLevel, y = Height)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot of Height by Education Level\",\n    x = \"Education Level\",\n    y = \"Height\"\n  )\n\n# Display the boxplot\nprint(p_boxplot)\n\n\n\n# Save the boxplot to a file\nggsave(\"boxplot.png\", plot = p_boxplot)\n\nSaving 7 x 5 in image\n\n# Create a scatterplot with Weight on the x-axis and Work Hours Per Week on the y-axis\np_scatterplot &lt;- ggplot(mydata, aes(x = Weight, y = WorkHoursPerWeek)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Weight and Work Hours Per Week\",\n    x = \"Weight\",\n    y = \"Work Hours Per Week\"\n  )\n\n# Display the scatterplot\nprint(p_scatterplot)\n\n\n\n# Save the scatterplot to a file\nggsave(\"scatterplot.png\", plot = p_scatterplot)\n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/sethharris/Desktop/DA-6833-02T Practicum II/sethharris-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/sethharris/Desktop/DA-6833-02T Practicum II/sethharris-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name`  `Variable Definition`                 `Allowed Values`       \n  &lt;chr&gt;            &lt;chr&gt;                                 &lt;chr&gt;                  \n1 Height           height in centimeters                 numeric value &gt;0 or NA \n2 Weight           weight in kilograms                   numeric value &gt;0 or NA \n3 Gender           identified gender (male/female/other) M/F/O/NA               \n4 WorkHoursPerWeek number of hours worked per week       numeric value &gt;0 or NA \n5 EducationLevel   highest level of education attained   None/HighSchool/Associ…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height           &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166…\n$ Weight           &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55…\n$ Gender           &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F…\n$ WorkHoursPerWeek &lt;chr&gt; \"40\", \"35\", \"20\", \"45\", \"50\", \"0\", \"30\", \"38\", \"25\", …\n$ EducationLevel   &lt;chr&gt; \"Bachelor\", \"Master\", \"HighSchool\", \"PhD\", \"Bachelor\"…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          WorkHoursPerWeek  \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n EducationLevel    \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender WorkHoursPerWeek EducationLevel\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;         \n1 180        80 M      40               Bachelor      \n2 175        70 O      35               Master        \n3 sixty      60 F      20               HighSchool    \n4 178        76 F      45               PhD           \n5 192        90 NA     50               Bachelor      \n6 6          55 F      0                None          \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n13\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n12\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n12\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n10\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n10\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n8\n0\n\n\nEducationLevel\n0\n1\n3\n9\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  }
]