[
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "First, I will create a synthetic dataset with multiple variables and some associations between them. I will then explore the data with plots and tables, and fit some simple models to see if they can recover the associations.\n\nGenerate Synthetic Dataset\n\n# Load necessary libraries\nlibrary(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate synthetic dataset\nn &lt;- 1000\nx1 &lt;- rnorm(n, mean = 5, sd = 2)\nx2 &lt;- rnorm(n, mean = 3, sd = 1)\nx3 &lt;- rbinom(n, 1, 0.5)\ny &lt;- 3 + 2 * x1 - 1.5 * x2 + 0.8 * x3 + rnorm(n)\n\n# Create data frame\ndf &lt;- data.frame(y, x1, x2, x3)\n\n# Explore the dataset\nsummary(df)\n\n       y                x1                x2                 x3       \n Min.   :-2.994   Min.   :-0.6195   Min.   :-0.04786   Min.   :0.000  \n 1st Qu.: 6.049   1st Qu.: 3.7434   1st Qu.: 2.34678   1st Qu.:0.000  \n Median : 8.906   Median : 5.0184   Median : 3.05485   Median :0.000  \n Mean   : 8.893   Mean   : 5.0323   Mean   : 3.04246   Mean   :0.489  \n 3rd Qu.:11.557   3rd Qu.: 6.3292   3rd Qu.: 3.75345   3rd Qu.:1.000  \n Max.   :22.691   Max.   :11.4821   Max.   : 6.39037   Max.   :1.000  \n\nhead(df)\n\n          y       x1        x2 x3\n1  7.072988 3.879049 2.0042013  0\n2 10.513535 4.539645 1.9600450  1\n3 14.857289 8.117417 2.9820198  1\n4  9.246312 5.141017 2.8678249  1\n5 13.615550 5.258575 0.4506572  0\n6 13.323778 8.430130 4.0405735  0\n\n\n\n\nExplore the Data\n\nPlot the relationships between variables\n\n# Plot y against x1\nggplot(df, aes(x = x1, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Scatter plot of y vs x1\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Plot y against x2\nggplot(df, aes(x = x2, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", col = \"blue\") +\n  labs(title = \"Scatter plot of y vs x2\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# Boxplot of y by x3\nggplot(df, aes(x = as.factor(x3), y = y)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of y by x3\", x = \"x3\")\n\n\n\n\n\nScatter plot of y vs x1: This plot shows a positive linear relationship between x1 and y. As x1 increases, y also tends to increase. The fitted line, shown in blue, confirms this positive correlation.\nScatter plot of y vs x2: This plot shows a slight negative linear relationship between x2 and y. As x2 increases, y tends to decrease slightly. The fitted line, shown in blue, indicates this negative correlation.\nBoxplot of y by x3: This boxplot shows the distribution of y for two categories of x3 (0 and 1). The median y value appears to be slightly higher for x3 = 1 compared to x3 = 0, indicating a potential difference in y based on the category of x3.\n\nOverall, the plots confirm the expected relationships in the synthetic data, with x1 having a positive association with y, x2 having a negative association with y, and x3 showing some differences in y based on its category.\nNow I will fit three linear models to explore the relationships in the synthetic dataset.\n\n\n\nFit Simple Models\n\nLinear Model\n\n# Fit a linear model\nmodel &lt;- lm(y ~ x1 + x2 + x3, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0422 -0.6685 -0.0369  0.6641  3.3416 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.00272    0.12866   23.34   &lt;2e-16 ***\nx1           1.98449    0.01603  123.83   &lt;2e-16 ***\nx2          -1.45520    0.03147  -46.24   &lt;2e-16 ***\nx3           0.67653    0.06333   10.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 996 degrees of freedom\nMultiple R-squared:  0.9436,    Adjusted R-squared:  0.9434 \nF-statistic:  5554 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nSummary:\n\nIntercept: 3.00272\nx1 coefficient: 1.98449\nx2 coefficient: -1.45520\nx3 coefficient: 0.67653\nMultiple R-squared: 0.9436\n\n\n\n\nInterpretation\n\nThe first model (linear) shows strong relationships with significant coefficients for all variables.\n\n\nExplore Model Coefficients\n\n# Print the coefficients of the linear model\ncat(\"Coefficients of the linear model:\\n\")\n\nCoefficients of the linear model:\n\nprint(coef(model))\n\n(Intercept)          x1          x2          x3 \n  3.0027214   1.9844910  -1.4551997   0.6765316 \n\n\n\n\n\nExplore Different Models\n\nPolynomial Regression\n\n# Fit a polynomial regression model\nmodel_poly &lt;- lm(y ~ poly(x1, 2) + poly(x2, 2) + x3, data = df)\nsummary(model_poly)\n\n\nCall:\nlm(formula = y ~ poly(x1, 2) + poly(x2, 2) + x3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0556 -0.6441 -0.0302  0.6627  3.3272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.56089    0.04433 193.105   &lt;2e-16 ***\npoly(x1, 2)1 124.40827    1.00551 123.726   &lt;2e-16 ***\npoly(x1, 2)2  -0.57825    1.00256  -0.577    0.564    \npoly(x2, 2)1 -46.44628    1.00515 -46.208   &lt;2e-16 ***\npoly(x2, 2)2  -0.22716    1.00122  -0.227    0.821    \nx3             0.67839    0.06346  10.690   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.001 on 994 degrees of freedom\nMultiple R-squared:  0.9436,    Adjusted R-squared:  0.9433 \nF-statistic:  3327 on 5 and 994 DF,  p-value: &lt; 2.2e-16\n\n\nSummary:\n\nIntercept: 8.56089\nx1 (1st term) coefficient: 124.40827\nx1 (2nd term) coefficient: -0.57825 (not significant)\nx2 (1st term) coefficient: -46.44628\nx2 (2nd term) coefficient: -0.22716 (not significant)\nx3 coefficient: 0.67839\nMultiple R-squared: 0.9436\n\n\n\n\nInterpretation\n\nThe second model (polynomial) adds quadratic terms for x1 and x2, but the additional complexity does not significantly improve the model, as the quadratic terms are not significant.\n\n\nInteraction Model\n\n# Fit a model with interaction terms\nmodel_interact &lt;- lm(y ~ x1 * x2 + x1 * x3 + x2 * x3, data = df)\nsummary(model_interact)\n\n\nCall:\nlm(formula = y ~ x1 * x2 + x1 * x3 + x2 * x3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0441 -0.6361 -0.0548  0.6617  3.3446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.67485    0.31034   8.619  &lt; 2e-16 ***\nx1           2.04367    0.05427  37.660  &lt; 2e-16 ***\nx2          -1.33398    0.09360 -14.251  &lt; 2e-16 ***\nx3           0.67439    0.24948   2.703  0.00698 ** \nx1:x2       -0.02194    0.01589  -1.381  0.16759    \nx1:x3        0.01434    0.03215   0.446  0.65569    \nx2:x3       -0.02219    0.06320  -0.351  0.72551    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.001 on 993 degrees of freedom\nMultiple R-squared:  0.9437,    Adjusted R-squared:  0.9434 \nF-statistic:  2775 on 6 and 993 DF,  p-value: &lt; 2.2e-16\n\n\nSummary:\n\nIntercept: 2.67485\nx1 coefficient: 2.04367\nx2 coefficient: -1.33398\nx3 coefficient: 0.67439\nx1\ninteraction: -0.02194 (not significant)\nx1\ninteraction: 0.01434 (not significant)\nx2\ninteraction: -0.02219 (not significant)\nMultiple R-squared: 0.9437\n\n\n\n\nInterpretation\n\nThe third model (interaction) introduces interaction terms, but they are not significant, indicating no substantial interaction effects between the variables.\n\n\n\nConclusion\nThis R code generates a synthetic dataset with multiple variables and known associations, explores the data with plots and tables, and fits several simple models to recover the associations. The linear model should reveal the coefficients that were built into the data generation process, while the polynomial and interaction models allow us to explore more complex relationships.\nOverall, the linear model successfully captures the main associations between the variables and y, while additional complexity from polynomial and interaction terms does not significantly enhance the model’s explanatory power."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Importing necessary libraries & packages\n\nlibrary(\"dslabs\") #loading dslabs package\nlibrary(\"tidyverse\") #loading tidyverse package\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"dplyr\") #loading dplyr package\nlibrary(\"ggplot2\") #loading ggplot2 package\ndata(\"gapminder\") #importing gapminder data\n\nThe dslabs package is an R package designed to provide datasets and functions for data science and statistics education. It includes a variety of real-world datasets that are useful for teaching and learning purposes.\nThe tidyverse is a collection of R packages designed for data science. All packages in the tidyverse share an underlying design philosophy, grammar, and data structures, which makes them work seamlessly together. The tidyverse includes packages for data manipulation, visualization, and modeling, and it’s particularly well-suited for data analysis tasks.\nKey packages in the tidyverse include:\n\nggplot2: For data visualization.\ndplyr: For data manipulation.\ntidyr: For data tidying.\nreadr: For data import.\npurrr: For functional programming.\ntibble: For modern data frames.\nstringr: For string manipulation.\nforcats: For working with categorical data (factors).\n\nThe dplyr package is a part of the tidyverse and provides a set of functions that are designed to simplify data manipulation in R. It is known for its ease of use and efficiency in handling data frames. Here are some key features and functions of the dplyr package:\n\nData Manipulation: dplyr allows you to perform a variety of data manipulation tasks, including filtering rows, selecting columns, reordering rows, and summarizing data.\nKey Functions:\n\nfilter(): Subsets rows based on their values.\nselect(): Subsets columns based on their names.\narrange(): Reorders rows.\nmutate(): Adds new columns or transforms existing ones.\nsummarize(): Aggregates data to produce a single summary statistic.\ngroup_by(): Groups data for grouped operations.\n\nChaining with Pipes: One of the most powerful features of dplyr is its compatibility with the pipe operator (%&gt;%). This allows you to chain multiple operations together in a clear and readable way.\nPerformance: dplyr functions are optimized for performance, making it efficient to work with large datasets.\nIntegration: dplyr integrates well with other tidyverse packages, making it easy to use in conjunction with packages like ggplot2 for visualization and tidyr for tidying data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "href": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Importing necessary libraries & packages\n\nlibrary(\"dslabs\") #loading dslabs package\nlibrary(\"tidyverse\") #loading tidyverse package\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"dplyr\") #loading dplyr package\nlibrary(\"ggplot2\") #loading ggplot2 package\ndata(\"gapminder\") #importing gapminder data\n\nThe dslabs package is an R package designed to provide datasets and functions for data science and statistics education. It includes a variety of real-world datasets that are useful for teaching and learning purposes.\nThe tidyverse is a collection of R packages designed for data science. All packages in the tidyverse share an underlying design philosophy, grammar, and data structures, which makes them work seamlessly together. The tidyverse includes packages for data manipulation, visualization, and modeling, and it’s particularly well-suited for data analysis tasks.\nKey packages in the tidyverse include:\n\nggplot2: For data visualization.\ndplyr: For data manipulation.\ntidyr: For data tidying.\nreadr: For data import.\npurrr: For functional programming.\ntibble: For modern data frames.\nstringr: For string manipulation.\nforcats: For working with categorical data (factors).\n\nThe dplyr package is a part of the tidyverse and provides a set of functions that are designed to simplify data manipulation in R. It is known for its ease of use and efficiency in handling data frames. Here are some key features and functions of the dplyr package:\n\nData Manipulation: dplyr allows you to perform a variety of data manipulation tasks, including filtering rows, selecting columns, reordering rows, and summarizing data.\nKey Functions:\n\nfilter(): Subsets rows based on their values.\nselect(): Subsets columns based on their names.\narrange(): Reorders rows.\nmutate(): Adds new columns or transforms existing ones.\nsummarize(): Aggregates data to produce a single summary statistic.\ngroup_by(): Groups data for grouped operations.\n\nChaining with Pipes: One of the most powerful features of dplyr is its compatibility with the pipe operator (%&gt;%). This allows you to chain multiple operations together in a clear and readable way.\nPerformance: dplyr functions are optimized for performance, making it efficient to work with large datasets.\nIntegration: dplyr integrates well with other tidyverse packages, making it easy to use in conjunction with packages like ggplot2 for visualization and tidyr for tidying data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#using-gapminder-dataset",
    "href": "coding-exercise/coding-exercise.html#using-gapminder-dataset",
    "title": "R Coding Exercise",
    "section": "Using Gapminder Dataset",
    "text": "Using Gapminder Dataset\nHelp() function to see what data contains\n\nhelp(gapminder) #looking at help file for gapminder data\n\nStr() and Summary() functions to take a look at the data\n\nstr(gapminder) #getting an overview of data structure\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n\n\nsummary(gapminder) #getting a summary of data\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n\nClass() function to check what type of object gapminder is\n\nclass(gapminder) #determining the type of object gapminder is\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing data",
    "text": "Processing data\nFilter the data: The filter function from dplyr is used to filter rows where the continent column equals “Africa”. The result is stored in the africadata object.\n\nafricadata &lt;- gapminder %&gt;% filter(continent == \"Africa\") #Filtering the data to include only African countries\n\nExplore africadata: We use the str and summary functions to display the structure and summary statistics of africadata.\n\nstr(africadata) #Displaying structure of the africadata object\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n\n\nsummary(africadata) #Displaying summary of the africadata object\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nCreate new objects: We create two new objects:\n\nafricadata_infant_life contains the infant_mortality and life_expectancy columns.\nafricadata_population_life contains the population and life_expectancy columns.\n\nExplore new objects: We use the str and summary functions to display the structure and summary statistics of both new objects.\n\nafricadata_infant_life &lt;- africadata %&gt;% select(infant_mortality, life_expectancy) #Creating a new object containing only 'infant_mortality' and 'life_expectancy'\n\nstr(africadata_infant_life) #Displaying structure of the new object containing only 'infant_mortality' and 'life_expectancy'\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\n\n\nsummary(africadata_infant_life) #Displaying summary of the new object containing only 'infant_mortality' and 'life_expectancy'\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\n\nafricadata_population_life &lt;- africadata %&gt;% select(population, life_expectancy) #Creating another new object containing only 'population' and 'life_expectancy'\n\nstr(africadata_population_life) #Displaying structure of the second new object containing only 'population' and 'life_expectancy'\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n\n\nsummary(africadata_population_life) #Displaying structure and summary of the second new object containing only 'population' and 'life_expectancy'\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting-to-analyze-key-relationships-in-african-countries-data",
    "href": "coding-exercise/coding-exercise.html#plotting-to-analyze-key-relationships-in-african-countries-data",
    "title": "R Coding Exercise",
    "section": "Plotting to Analyze Key Relationships in African Countries Data",
    "text": "Plotting to Analyze Key Relationships in African Countries Data\n\nplot1 &lt;- ggplot(africadata_infant_life, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color=\"darkgreen\") +\n  labs(title = \"Life Expectancy vs Infant Mortality in African countries\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\") +\n  theme_minimal() \n#Plotting life expectancy as a function of infant mortality for African countries\n\nprint(plot1)\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n#Displaying the first plot\n\nExplanation of the Observations\nLife Expectancy vs Infant Mortality:\n\nIn the first plot, we observe a negative correlation between infant mortality and life expectancy. This means that as infant mortality increases, life expectancy tends to decrease. This relationship makes sense because high infant mortality rates are often associated with poor healthcare, inadequate nutrition, and other socio-economic factors that negatively impact overall life expectancy.\nThe points are colored in dark green for better visual distinction.\n\n\nplot2 &lt;- ggplot(africadata_population_life, aes(x = population, y = life_expectancy)) +\n  geom_point(color=\"red\") +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs Population Size in African Countries\",\n       x = \"Population Size (log scale)\",\n       y = \"Life Expectancy\") +\n  theme_minimal()\n#Plotting life expectancy as a function of population size (x-axis in log scale) for African countries\n#Displaying the second plot\nprint(plot2)\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nExplanation of the Observations\nLife Expectancy vs Population Size:\n\nIn the second plot, we observe a positive correlation between population size and life expectancy. This suggests that countries with larger populations tend to have higher life expectancy. One reason for this could be that larger populations often have better infrastructure, healthcare systems, and resources that contribute to higher life expectancy.\nThe plot uses a log scale for the x-axis to better visualize the wide range of population sizes. Using a log scale helps to spread out the data points and makes it easier to see patterns and relationships.\nThe points are colored in red for better visual distinction.\nWe also notice streaks of data that seem to group together. These streaks may indicate that there are clusters of countries with similar population sizes and life expectancy. Taking a closer look at the africadata dataset, we may find that these clusters represent countries with similar socio-economic conditions, healthcare access, and other factors that influence life expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-data-processing",
    "href": "coding-exercise/coding-exercise.html#more-data-processing",
    "title": "R Coding Exercise",
    "section": "More data processing",
    "text": "More data processing\nExtracting Data for Year 2000\n\nmissing_years &lt;- africadata[is.na(africadata$infant_mortality), \"year\"]\n#Identifying years with missing data for infant mortality\n\nIdentify Years with Missing Data:\n\nWe first identify the years with missing data for the infant_mortality variable by filtering rows where infant_mortality is NA.\nWe then get the unique years with missing data and print them to identify which years to avoid.\n\n\nunique_missing_years &lt;- unique(missing_years)\nprint(unique_missing_years)\n\n [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974\n[16] 1975 1976 1977 1978 1979 1980 1981 2016\n\n#Displaying unique years with missing data\n\n\nafricadata_2000 &lt;- africadata[africadata$year == 2000, ]\n#Extracting data for the year 2000\n\nExtract Data for the Year 2000:\n\nWe extract only the rows from africadata where the year is 2000.\nWe assign this filtered data to a new object called africadata_2000.\n\n\nstr(africadata_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n#Checking the structure of the new dataset\n\n\nsummary(africadata_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n#Checking the summary of the new dataset\n\nCheck Structure and Summary:\nWe use the str and summary functions to check the structure and summary statistics of the new dataset africadata_2000.\n\nThe new dataset should have 51 observations and 9 variables.\n\nBy extracting the data for the year 2000, we can now analyze the patterns without the confounding effects of different years. This allows us to focus on the relationship between variables for a single year, providing clearer insights into the data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting",
    "href": "coding-exercise/coding-exercise.html#more-plotting",
    "title": "R Coding Exercise",
    "section": "More plotting",
    "text": "More plotting\n\nafricadata_2000 &lt;- africadata[africadata$year == 2000, ]\n#Creating a new object with data for the year 2000\n\np_infant_mortality_2000 &lt;- ggplot(africadata_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color = \"darkgreen\") +\n  labs(title = \"Life Expectancy vs. Infant Mortality (Year 2000) in African Countries\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\")\n#Plotting life expectancy as a function of infant mortality in African countries\n\nprint(p_infant_mortality_2000)\n\n\n\n#Displaying the plot\n\nLife Expectancy vs. Infant Mortality (Year 2000) in African Countries:\nThe scatter plot shows a negative correlation between infant mortality and life expectancy. As infant mortality decreases, life expectancy tends to increase. This makes sense as lower infant mortality rates often indicate better healthcare and living conditions, leading to longer life expectancy.\n\np_population_2000 &lt;- ggplot(africadata_2000, aes(x = population, y = life_expectancy)) +\n  geom_point(color = \"red\") +\n  scale_x_log10() +  # Set x-axis to log scale\n  labs(title = \"Life Expectancy vs. Population Size (Year 2000) in African Countries\",\n       x = \"Population (Log Scale)\",\n       y = \"Life Expectancy\")\n#Plotting life expectancy as a function of population size in African countries\n\nprint(p_population_2000)\n\n\n\n#Displaying the plot\n\nLife Expectancy vs. Population Size (Year 2000) in African Countries:\nThe scatter plot with the population size on a log scale shows no noticeable correlation between population size and life expectancy. This indicates that population size does not have a strong direct impact on life expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-model-fits",
    "href": "coding-exercise/coding-exercise.html#simple-model-fits",
    "title": "R Coding Exercise",
    "section": "Simple model fits",
    "text": "Simple model fits\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_2000)\n#Fitting the first model with life expectancy as the outcome and infant mortality as the predictor\n\nsummary_fit1 &lt;- summary(fit1)\n#Applying the summary command to the first fit\n\nprint(summary_fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\nExplanation and Interpretation\nModel 1: Life Expectancy vs. Infant Mortality\nFit 1 Summary:\nsummary_fit1\nThis command provides detailed statistics about the linear regression model. Key metrics include the coefficients, standard errors, t-values, and p-values for the predictor (infant_mortality) and the intercept. Additionally, it shows the R-squared value, which indicates the proportion of variance in the outcome (life_expectancy) explained by the predictor.\nInterpretation:\n\nThe p-value for the infant mortality predictor is likely to be very small (p-value &lt; 0.05), indicating a statistically significant relationship between infant mortality and life expectancy.\nThe negative coefficient for infant mortality suggests that as infant mortality increases, life expectancy decreases, which aligns with the negative correlation observed in the scatter plot.\n\n\nfit2 &lt;- lm(life_expectancy ~ (population), data = africadata_2000)\n#Fitting the second model with life expectancy as the outcome and population size as the predictor\n\nsummary_fit2 &lt;- summary(fit2)\n#Applying the summary command to the second fit\nprint(summary_fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ (population), data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nExplanation and Interpretation\nModel 2: Life Expectancy vs. Population Size\nFit 2 Summary:\nsummary_fit2\nThis command provides similar statistics for the linear regression model with population size as the predictor.\nInterpretation:\nThe p-value for the population predictor is likely to be large (p-value &gt; 0.05), indicating that there is no statistically significant relationship between population size and life expectancy.\nThe coefficient for the transformed population size may not provide a meaningful interpretation due to the lack of statistical significance."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#conclusion",
    "href": "coding-exercise/coding-exercise.html#conclusion",
    "title": "R Coding Exercise",
    "section": "Conclusion",
    "text": "Conclusion\nInfant Mortality as Predictor:\n\nThere is a statistically significant negative relationship between infant mortality and life expectancy. This suggests that higher infant mortality rates are associated with lower life expectancy.\n\nPopulation Size as Predictor:\n\nThere is no statistically significant relationship between population size and life expectancy. Population size does not appear to be a strong predictor of life expectancy based on the 2000 data.\n\nBy fitting these simple models and interpreting the results, we gain insights into the relationships between life expectancy, infant mortality, and population size. This process highlights the importance of using statistical models to understand data patterns and make informed conclusions."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-contributed-by-collin-real",
    "href": "coding-exercise/coding-exercise.html#this-section-contributed-by-collin-real",
    "title": "R Coding Exercise",
    "section": "This section contributed by Collin Real",
    "text": "This section contributed by Collin Real"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#explore-the-dataset",
    "href": "coding-exercise/coding-exercise.html#explore-the-dataset",
    "title": "R Coding Exercise",
    "section": "Explore the dataset",
    "text": "Explore the dataset\nStr() and Summary() functions to take a look at the data\n\ndata(polls_us_election_2016) # import data\n\n\nhelp(polls_us_election_2016)\n\n\nstr(polls_us_election_2016) #getting an overview of data structure\n\n'data.frame':   4208 obs. of  15 variables:\n $ state           : Factor w/ 57 levels \"Alabama\",\"Alaska\",..: 50 50 50 50 50 50 50 50 37 50 ...\n $ startdate       : Date, format: \"2016-11-03\" \"2016-11-01\" ...\n $ enddate         : Date, format: \"2016-11-06\" \"2016-11-07\" ...\n $ pollster        : Factor w/ 196 levels \"ABC News/Washington Post\",..: 1 63 81 194 65 55 18 113 195 76 ...\n $ grade           : Factor w/ 10 levels \"D\",\"C-\",\"C\",\"C+\",..: 10 6 8 6 5 9 8 8 NA 8 ...\n $ samplesize      : int  2220 26574 2195 3677 16639 1295 1426 1282 8439 1107 ...\n $ population      : chr  \"lv\" \"lv\" \"lv\" \"lv\" ...\n $ rawpoll_clinton : num  47 38 42 45 47 ...\n $ rawpoll_trump   : num  43 35.7 39 41 43 ...\n $ rawpoll_johnson : num  4 5.46 6 5 3 3 5 6 6 7.1 ...\n $ rawpoll_mcmullin: num  NA NA NA NA NA NA NA NA NA NA ...\n $ adjpoll_clinton : num  45.2 43.3 42 45.7 46.8 ...\n $ adjpoll_trump   : num  41.7 41.2 38.8 40.9 42.3 ...\n $ adjpoll_johnson : num  4.63 5.18 6.84 6.07 3.73 ...\n $ adjpoll_mcmullin: num  NA NA NA NA NA NA NA NA NA NA ...\n\n\n\nsummary(polls_us_election_2016) #getting a summary of data\n\n            state        startdate             enddate          \n U.S.          :1106   Min.   :2015-11-06   Min.   :2015-11-08  \n Florida       : 148   1st Qu.:2016-08-10   1st Qu.:2016-08-21  \n North Carolina: 125   Median :2016-09-23   Median :2016-09-30  \n Pennsylvania  : 125   Mean   :2016-08-31   Mean   :2016-09-06  \n Ohio          : 115   3rd Qu.:2016-10-20   3rd Qu.:2016-10-28  \n New Hampshire : 112   Max.   :2016-11-06   Max.   :2016-11-07  \n (Other)       :2477                                            \n                                     pollster        grade     \n Ipsos                                   : 919   A-     :1085  \n Google Consumer Surveys                 : 743   B      :1011  \n SurveyMonkey                            : 660   C-     : 693  \n YouGov                                  : 130   C+     : 329  \n Rasmussen Reports/Pulse Opinion Research: 125   B+     : 204  \n USC Dornsife/LA Times                   : 121   (Other): 457  \n (Other)                                 :1510   NA's   : 429  \n   samplesize       population        rawpoll_clinton rawpoll_trump  \n Min.   :   35.0   Length:4208        Min.   :11.04   Min.   : 4.00  \n 1st Qu.:  447.5   Class :character   1st Qu.:38.00   1st Qu.:35.00  \n Median :  772.0   Mode  :character   Median :43.00   Median :40.00  \n Mean   : 1148.2                      Mean   :41.99   Mean   :39.83  \n 3rd Qu.: 1236.5                      3rd Qu.:46.20   3rd Qu.:45.00  \n Max.   :84292.0                      Max.   :88.00   Max.   :68.00  \n NA's   :1                                                           \n rawpoll_johnson  rawpoll_mcmullin adjpoll_clinton adjpoll_trump   \n Min.   : 0.000   Min.   : 9.0     Min.   :17.06   Min.   : 4.373  \n 1st Qu.: 5.400   1st Qu.:22.5     1st Qu.:40.21   1st Qu.:38.429  \n Median : 7.000   Median :25.0     Median :44.15   Median :42.765  \n Mean   : 7.382   Mean   :24.0     Mean   :43.32   Mean   :42.674  \n 3rd Qu.: 9.000   3rd Qu.:27.9     3rd Qu.:46.92   3rd Qu.:46.290  \n Max.   :25.000   Max.   :31.0     Max.   :86.77   Max.   :72.433  \n NA's   :1409     NA's   :4178                                     \n adjpoll_johnson  adjpoll_mcmullin\n Min.   :-3.668   Min.   :11.03   \n 1st Qu.: 3.145   1st Qu.:23.11   \n Median : 4.384   Median :25.14   \n Mean   : 4.660   Mean   :24.51   \n 3rd Qu.: 5.756   3rd Qu.:27.98   \n Max.   :20.367   Max.   :31.57   \n NA's   :1409     NA's   :4178"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#pre-process-data",
    "href": "coding-exercise/coding-exercise.html#pre-process-data",
    "title": "R Coding Exercise",
    "section": "Pre Process Data",
    "text": "Pre Process Data\n\n# Filter by U.S. state greater than October 31, 2016 and remove null values from grade\ndf = polls_us_election_2016 |&gt; \n  filter(state == \"U.S.\" & enddate &gt;= \"2016-10-31\" &\n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade)))\nhead(df, 10)\n\n   state  startdate    enddate\n1   U.S. 2016-11-03 2016-11-06\n2   U.S. 2016-11-02 2016-11-06\n3   U.S. 2016-11-03 2016-11-06\n4   U.S. 2016-11-02 2016-11-06\n5   U.S. 2016-11-03 2016-11-05\n6   U.S. 2016-11-04 2016-11-07\n7   U.S. 2016-11-04 2016-11-06\n8   U.S. 2016-11-01 2016-11-04\n9   U.S. 2016-11-03 2016-11-06\n10  U.S. 2016-11-01 2016-11-03\n                                                     pollster grade samplesize\n1                                    ABC News/Washington Post    A+       2220\n2                                                       Ipsos    A-       2195\n3  Fox News/Anderson Robbins Research/Shaw & Company Research     A       1295\n4                                     CBS News/New York Times    A-       1426\n5                                NBC News/Wall Street Journal    A-       1282\n6                                                    IBD/TIPP    A-       1107\n7                                            Selzer & Company    A+        799\n8                                           Angus Reid Global    A-       1151\n9                                         Monmouth University    A+        748\n10                                             Marist College     A        940\n   population rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin\n1          lv            47.0          43.0             4.0               NA\n2          lv            42.0          39.0             6.0               NA\n3          lv            48.0          44.0             3.0               NA\n4          lv            45.0          41.0             5.0               NA\n5          lv            44.0          40.0             6.0               NA\n6          lv            41.2          42.7             7.1               NA\n7          lv            44.0          41.0             4.0               NA\n8          lv            48.0          44.0             6.0               NA\n9          lv            50.0          44.0             4.0               NA\n10         lv            44.0          43.0             6.0               NA\n   adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin\n1         45.20163      41.72430        4.626221               NA\n2         42.02638      38.81620        6.844734               NA\n3         49.02208      43.95631        3.057876               NA\n4         45.11649      40.92722        4.341786               NA\n5         43.58576      40.77325        5.365788               NA\n6         42.92745      42.23545        6.316175               NA\n7         44.21714      40.57082        4.068708               NA\n8         47.57171      43.68125        5.556625               NA\n9         48.86765      43.39600        4.838600               NA\n10        42.83406      43.43819        4.780429               NA"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#visualize-distribution-of-data",
    "href": "coding-exercise/coding-exercise.html#visualize-distribution-of-data",
    "title": "R Coding Exercise",
    "section": "Visualize Distribution of Data",
    "text": "Visualize Distribution of Data\n\nBoxplots - Trump Polls\n\nggplot(df, aes(x = grade, y = rawpoll_trump)) +\n  geom_boxplot(fill='steelblue', color='black') +\n  labs(title = \"Trump Boxplot - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n\n\n\n\n\nBoxplots - Clinton Polls\n\nggplot(df, aes(x = grade, y = rawpoll_clinton)) +\n  geom_boxplot(fill='steelblue', color='black') +\n  labs(title = \"Clinton Boxplot - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n\n\n\n\n\nHistogram - Trump Polls\n\ndf %&gt;%\n  ggplot(aes(x = rawpoll_trump)) +\n  geom_histogram(color='black', fill='steelblue') +\n  labs(title = \"Trump Histogram - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nHistogram - Hillary Polls\n\n# Create a boxplot of price distribution by electric vehicle type\ndf %&gt;%\n  select(grade, rawpoll_clinton) %&gt;%\n  ggplot(aes(x = rawpoll_clinton)) +\n  geom_histogram(color='black', fill='steelblue') +\n  labs(title = \"Trump Histogram - Poll % Distribution by Grade\",\n       x = \"Grade\",\n       y = \"Poll % Distribution\") + \n  facet_wrap(~grade, scales = 'free')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nFound this snippet from an online tutorial\n\npolls_us_election_2016 %&gt;%\n  filter(state == \"U.S.\" & enddate&gt;=\"2016-07-01\") %&gt;%\n  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %&gt;%\n  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %&gt;%\n  gather(candidate, percentage, -enddate, -pollster) %&gt;% \n  mutate(candidate = factor(candidate, levels = c(\"Trump\",\"Clinton\")))%&gt;%\n  group_by(pollster) %&gt;%\n  filter(n()&gt;=10) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(enddate, percentage, color = candidate)) +  \n  geom_point(show.legend = FALSE, alpha=0.4)  + \n  geom_smooth(method = \"loess\", span = 0.15) +\n  scale_y_continuous(limits = c(30,50))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 22 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 22 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\nStatistical Models\n\n\nPredicting Electoral College Votes\n\nTop 5 States w/ Most Electoral Votes\nRemoved my initial models in favor of this comprehensive example I found online - more insightful tutorial than my basic regressions\n\nresults_us_election_2016 %&gt;% top_n(5, electoral_votes)\n\n         state electoral_votes clinton trump others\n1   California              55    61.7  31.6    6.7\n2        Texas              38    43.2  52.2    4.5\n3      Florida              29    47.8  49.0    3.2\n4     New York              29    59.0  36.5    4.5\n5     Illinois              20    55.8  38.8    5.4\n6 Pennsylvania              20    47.9  48.6    3.6\n\n\n\n\n\nAggregrate Poll Results - Week Before Election\n\nresults &lt;- polls_us_election_2016 %&gt;%\n  filter(state!=\"U.S.\" & \n           !grepl(\"CD\", state) & \n           enddate &gt;=\"2016-10-31\" & \n           (grade %in% c(\"A+\",\"A\",\"A-\",\"B+\") | is.na(grade))) %&gt;%\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&gt;%\n  group_by(state) %&gt;%\n  summarize(avg = mean(spread), sd = sd(spread), n = n()) %&gt;%\n  mutate(state = as.character(state))\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 4\n$ state &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Con…\n$ avg   &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000, 0.0452000…\n$ sd    &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317, 0.029459…\n$ n     &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, 3, 4, 6, 3…\n\n\n\n\nJoin the number of electoral votes for each state\n\nresults &lt;- left_join(results, results_us_election_2016, by = \"state\")\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 8\n$ state           &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Color…\n$ avg             &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000,…\n$ sd              &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317…\n$ n               &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, …\n$ electoral_votes &lt;int&gt; 9, 11, 6, 55, 9, 7, 3, 29, 16, 4, 4, 20, 11, 6, 6, 8, …\n$ clinton         &lt;dbl&gt; 34.4, 45.1, 33.7, 61.7, 48.2, 54.6, 53.4, 47.8, 45.9, …\n$ trump           &lt;dbl&gt; 62.1, 48.7, 60.6, 31.6, 43.3, 40.9, 41.9, 49.0, 51.0, …\n$ others          &lt;dbl&gt; 3.6, 6.2, 5.8, 6.7, 8.6, 4.5, 4.7, 3.2, 3.1, 7.7, 13.2…\n\n\n\n\nStates with no poll data - high confidence of winner\n\nresults_us_election_2016 %&gt;% filter(!state %in% results$state) %&gt;% \n  pull(state)\n\n[1] \"Rhode Island\"         \"Alaska\"               \"Wyoming\"             \n[4] \"District of Columbia\"\n\n\n\n\nEstimate standard deviation\n\nresults &lt;- results %&gt;%\n  mutate(sd = ifelse(is.na(sd), median(results$sd, na.rm = TRUE), sd))\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 8\n$ state           &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Color…\n$ avg             &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000,…\n$ sd              &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317…\n$ n               &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, …\n$ electoral_votes &lt;int&gt; 9, 11, 6, 55, 9, 7, 3, 29, 16, 4, 4, 20, 11, 6, 6, 8, …\n$ clinton         &lt;dbl&gt; 34.4, 45.1, 33.7, 61.7, 48.2, 54.6, 53.4, 47.8, 45.9, …\n$ trump           &lt;dbl&gt; 62.1, 48.7, 60.6, 31.6, 43.3, 40.9, 41.9, 49.0, 51.0, …\n$ others          &lt;dbl&gt; 3.6, 6.2, 5.8, 6.7, 8.6, 4.5, 4.7, 3.2, 3.1, 7.7, 13.2…\n\n\n\n\nSimulate election day voting with Monte Carlo\n\nmu &lt;- 0\ntau &lt;- 0.02\nresults &lt;- results %&gt;% mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1/ (1/sigma^2 + 1/tau^2)))\nresults %&gt;% glimpse()   \n\nRows: 47\nColumns: 12\n$ state           &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Color…\n$ avg             &lt;dbl&gt; -0.149433333, -0.032644444, -0.151400000, 0.260100000,…\n$ sd              &lt;dbl&gt; 0.0253279161, 0.0269547357, 0.0009899495, 0.0387329317…\n$ n               &lt;int&gt; 3, 9, 2, 5, 7, 3, 2, 7, 4, 1, 2, 3, 3, 3, 2, 3, 2, 2, …\n$ electoral_votes &lt;int&gt; 9, 11, 6, 55, 9, 7, 3, 29, 16, 4, 4, 20, 11, 6, 6, 8, …\n$ clinton         &lt;dbl&gt; 34.4, 45.1, 33.7, 61.7, 48.2, 54.6, 53.4, 47.8, 45.9, …\n$ trump           &lt;dbl&gt; 62.1, 48.7, 60.6, 31.6, 43.3, 40.9, 41.9, 49.0, 51.0, …\n$ others          &lt;dbl&gt; 3.6, 6.2, 5.8, 6.7, 8.6, 4.5, 4.7, 3.2, 3.1, 7.7, 13.2…\n$ sigma           &lt;dbl&gt; 0.014623079, 0.008984912, 0.000700000, 0.017321894, 0.…\n$ B               &lt;dbl&gt; 0.348358497, 0.167929753, 0.001223501, 0.428610610, 0.…\n$ posterior_mean  &lt;dbl&gt; -0.097376962, -0.027162471, -0.151214762, 0.148618380,…\n$ posterior_se    &lt;dbl&gt; 0.0118043805, 0.0081958466, 0.0006995716, 0.0130936719…\n\n\n\n\nSimulate 10,000 times\n\nB &lt;- 10000\nmu &lt;- 0\ntau &lt;- 0.02\nclinton_EV = replicate(B, {\n  results %&gt;% mutate(sigma = sd/sqrt(n), \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B * mu + (1 - B) * avg,\n                   posterior_se = sqrt(1 / (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result &gt; 0, electoral_votes, 0)) %&gt;%\n    summarize(clinton = sum(clinton)) %&gt;%\n    pull(clinton) + 7\n})\nmean(clinton_EV &gt; 269)\n\n[1] 0.9978\n\n\n\n\nInclude general bias\n\ntau &lt;- 0.02\nbias_sd &lt;- 0.03\nclinton_EV_2 &lt;- replicate(1000, {\n  results %&gt;% mutate(sigma = sqrt(sd^2/n  + bias_sd^2),  \n                   B = sigma^2 / (sigma^2 + tau^2),\n                   posterior_mean = B*mu + (1-B)*avg,\n                   posterior_se = sqrt( 1/ (1/sigma^2 + 1/tau^2)),\n                   result = rnorm(length(posterior_mean), \n                                  posterior_mean, posterior_se),\n                   clinton = ifelse(result&gt;0, electoral_votes, 0)) %&gt;% \n    summarize(clinton = sum(clinton) + 7) %&gt;% \n    pull(clinton)\n})\nmean(clinton_EV_2 &gt; 269)\n\n[1] 0.844\n\n\n\nhist(clinton_EV)\n\n\n\nhist(clinton_EV_2)"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#modeling-results",
    "href": "coding-exercise/coding-exercise.html#modeling-results",
    "title": "R Coding Exercise",
    "section": "Modeling Results",
    "text": "Modeling Results\nTwo models were ran to predict the amount of electoral votes Hillary Clinton would receive in the 2016 election. The first model concluded that Clinton had a 99% chance of winning the presidency with more than 269 electoral votes. However, the first model assumes results from different states are independent, ignoring the general bias. Model 2 addresses these flaws, calculating an 82.5% chance of Clinton winning the presidency. The histograms illustrate Model 2’s robustness compared to the first model, showing how including bias adds more variability in the final predictions."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "aboutme.html#cool-data-analysis-resources",
    "href": "aboutme.html#cool-data-analysis-resources",
    "title": "About Me",
    "section": "Cool Data Analysis Resources",
    "text": "Cool Data Analysis Resources\nI recently came across this fascinating video on Youtube: What does data and analytics have to do with shoes? that explains how data analytics is used in a major sporting brand, such as Nike. It’s an excellent example of how data analysis can help inspire new ideas and trends for all professional athletes around the world! In addition, I found this Harvard Article: Nike: It’s Data Analytics, Just Do It that highlights the digital innovation and transformation used in the Nike brand."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Howdy y’all!\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nFeel free to navigate around my website on the various sections shown above:\n\nAbout Me: Learn more about my background, education, and interests.\nProjects: Check out some of the exciting data analysis projects I’ve worked on during my DA 6833 Practicum II course.\n\nConnect with me on LinkedIn: Stay in touch and follow my professional journey!\n\nThank you for visiting, and I hope you find my work both interesting and informative!"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Placeholder file for the future data/results presentation exercise."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "title: “Manuscript/Report Template for a Data Analysis Project” subtitle: “” author: Seth Harris date: today format: html: toc: false number-sections: true highlight-style: github bibliography: ../dataanalysis-template-references.bib csl: ../apa.csl"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#joaquin-ramirez-contributed-to-this-exercise",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#joaquin-ramirez-contributed-to-this-exercise",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "0.1 JOAQUIN RAMIREZ CONTRIBUTED TO THIS EXERCISE",
    "text": "0.1 JOAQUIN RAMIREZ CONTRIBUTED TO THIS EXERCISE\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. The exampledata.xlsx is the original dataset given. The dataset is composed of several variables, such as Height, Weight and Gender columns of a sample population. I added two more columns to the original dataset, one being numerical and the other being categorical. This data demonstrates data analysis that is used in R. My two added columns were WorkHoursPerWeek and EducationLevel to demonstrate the number of hours worked per week (numerical) and highest level of education attained (categorical) with the descriptions of my new variables being added in the Codebook sheet. I then saved it as exampledata2.xlsx."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n8\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nEducationLevel\n0\n1\n3\n9\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(here)\n\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n# Create a boxplot of Height by Education Level\np_boxplot &lt;- ggplot(mydata, aes(x = EducationLevel, y = Height)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot of Height by Education Level\",\n    x = \"Education Level\",\n    y = \"Height\"\n  )\n\n# Display the boxplot\nprint(p_boxplot)\n\n\n\n# Save the boxplot to a file\nggsave(\"boxplot.png\", plot = p_boxplot)\n\nSaving 7 x 5 in image\n\n# Create a scatterplot with Weight on the x-axis and Work Hours Per Week on the y-axis\np_scatterplot &lt;- ggplot(mydata, aes(x = Weight, y = WorkHoursPerWeek)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Weight and Work Hours Per Week\",\n    x = \"Weight\",\n    y = \"Work Hours Per Week\"\n  )\n\n# Display the scatterplot\nprint(p_scatterplot)\n\n\n\n# Save the scatterplot to a file\nggsave(\"scatterplot.png\", plot = p_scatterplot)\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\n\n\nTable 3: Another linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n183\n9.899495\n18.4857916\n0.0344048\n\n\nWorkHoursPerWeek15\n-50\n14.000000\n-3.5714286\n0.1738027\n\n\nWorkHoursPerWeek30\n-27\n14.000000\n-1.9285714\n0.3045286\n\n\nWorkHoursPerWeek35\n-8\n14.000000\n-0.5714286\n0.6695013\n\n\nWorkHoursPerWeek37\n-29\n14.000000\n-2.0714286\n0.2863259\n\n\nWorkHoursPerWeek38\n-17\n14.000000\n-1.2142857\n0.4385829\n\n\nWorkHoursPerWeek40\n-10\n12.124356\n-0.8247861\n0.5609407\n\n\nWorkHoursPerWeek45\n-5\n14.000000\n-0.3571429\n0.7816242\n\n\nEducationLevelBachelor\nNA\nNA\nNA\nNA\n\n\nEducationLevelMaster\nNA\nNA\nNA\nNA\n\n\nEducationLevelNone\nNA\nNA\nNA\nNA\n\n\nEducationLevelPhD\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nThe boxplot shows that individuals with a Bachelor’s degree tend to have the highest median height, followed by those with a Master’s degree. Individuals with an Associate’s degree have the lowest median height. There are also notable outliers among those with an Associate’s degree. The categories “None” and “PhD” have very limited data, showing a narrower range of heights.\nThe scatterplot shows no linear relationship between the variables. Data points are scattered widely across the plot, indicating that weight does not consistently predict the number of work hours per week for the individuals in the dataset.\nresulttable3.rds - output: The linear model fit table shows that the intercept is significant with a p-value of 0.034. However, none of the work hours per week predictors are statistically significant, with p-values ranging from 0.17 to 0.78, and the education level predictors were not included in the model due to missing data."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Conclusions",
    "text": "6.1 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/srilakshmi/Documents/GitHub/sethharris-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "href": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/srilakshmi/Documents/GitHub/sethharris-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                2     \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable    n_missing complete_rate min max empty n_unique whitespace\n1 WorkHoursPerWeek         0             1   1   2     0        8          0\n2 EducationLevel           0             1   3   9     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a boxplot of Height by Education Level\np_boxplot &lt;- ggplot(mydata, aes(x = EducationLevel, y = Height)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot of Height by Education Level\",\n    x = \"Education Level\",\n    y = \"Height\"\n  )\n\n# Display the boxplot\nprint(p_boxplot)\n\n\n\n# Save the boxplot to a file\nggsave(\"boxplot.png\", plot = p_boxplot)\n\nSaving 7 x 5 in image\n\n# Create a scatterplot with Weight on the x-axis and Work Hours Per Week on the y-axis\np_scatterplot &lt;- ggplot(mydata, aes(x = Weight, y = WorkHoursPerWeek)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Weight and Work Hours Per Week\",\n    x = \"Weight\",\n    y = \"Work Hours Per Week\"\n  )\n\n# Display the scatterplot\nprint(p_scatterplot)\n\n\n\n# Save the scatterplot to a file\nggsave(\"scatterplot.png\", plot = p_scatterplot)\n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/srilakshmi/Documents/GitHub/sethharris-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/srilakshmi/Documents/GitHub/sethharris-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name`  `Variable Definition`                 `Allowed Values`       \n  &lt;chr&gt;            &lt;chr&gt;                                 &lt;chr&gt;                  \n1 Height           height in centimeters                 numeric value &gt;0 or NA \n2 Weight           weight in kilograms                   numeric value &gt;0 or NA \n3 Gender           identified gender (male/female/other) M/F/O/NA               \n4 WorkHoursPerWeek number of hours worked per week       numeric value &gt;0 or NA \n5 EducationLevel   highest level of education attained   None/HighSchool/Associ…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height           &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166…\n$ Weight           &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55…\n$ Gender           &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F…\n$ WorkHoursPerWeek &lt;chr&gt; \"40\", \"35\", \"20\", \"45\", \"50\", \"0\", \"30\", \"38\", \"25\", …\n$ EducationLevel   &lt;chr&gt; \"Bachelor\", \"Master\", \"HighSchool\", \"PhD\", \"Bachelor\"…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          WorkHoursPerWeek  \n Length:14          Min.   :  45.0   Length:14          Length:14         \n Class :character   1st Qu.:  55.0   Class :character   Class :character  \n Mode  :character   Median :  70.0   Mode  :character   Mode  :character  \n                    Mean   : 602.7                                        \n                    3rd Qu.:  90.0                                        \n                    Max.   :7000.0                                        \n                    NA's   :1                                             \n EducationLevel    \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender WorkHoursPerWeek EducationLevel\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;         \n1 180        80 M      40               Bachelor      \n2 175        70 O      35               Master        \n3 sixty      60 F      20               HighSchool    \n4 178        76 F      45               PhD           \n5 192        90 NA     50               Bachelor      \n6 6          55 F      0                None          \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n13\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n12\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n12\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n10\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n10\n0\n\n\nEducationLevel\n0\n1\n3\n10\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nWorkHoursPerWeek\n0\n1\n1\n2\n0\n8\n0\n\n\nEducationLevel\n0\n1\n3\n9\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Seth Harris Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exploration on Weekly United States Hospitalization Metrics by Jurisdiction",
    "section": "",
    "text": "Brief Description of the Data\nTitle: Weekly United States Hospitalization Metrics by Jurisdiction, During Mandatory Reporting Period from August 1, 2020 to April 30, 2024, and for Data Reported Voluntarily Beginning May 1, 2024, National Healthcare Safety Network (NHSN)\nSource: (https://data.cdc.gov/Public-Health-Surveillance/Weekly-United-States-Hospitalization-Metrics-by-Ju/aemt-mg7g/about_data)\nDescription: This dataset provides information on COVID-19 and influenza-related hospitalizations, hospital occupancy, and hospital capacity across different jurisdictions in the United States from August 1, 2020, to April 30, 2024, and data reported voluntarily from May 1, 2024. It includes both continuous and categorical variables. I will further explore the dataset from the CDC data website. Specifically looking at information on hospitalization metrics, including the number of new admissions, total number of beds occupied, and more.\n\n\nLoading and Processing the Data\nThe first step is to load the data and perform some initial cleaning and processing.\n\n# Load necessary libraries \nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n\n# Load the dataset\ncdcdata &lt;- read_csv(\"https://data.cdc.gov/api/views/aemt-mg7g/rows.csv?accessType=DOWNLOAD\")\n\nRows: 11571 Columns: 64\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): Geographic aggregation\ndbl  (62): Hospital Reporting Days, Percent Hospital Reporting Days, Number ...\ndate  (1): Week Ending Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Inspect the first few rows of the data and column names\nhead(cdcdata)\n\n# A tibble: 6 × 64\n  `Week Ending Date` `Geographic aggregation` `Hospital Reporting Days`\n  &lt;date&gt;             &lt;chr&gt;                                        &lt;dbl&gt;\n1 2020-08-08         AL                                             629\n2 2020-08-15         AL                                             637\n3 2020-08-22         AL                                             632\n4 2020-08-29         AL                                             636\n5 2020-09-05         AL                                             640\n6 2020-09-12         AL                                             637\n# ℹ 61 more variables: `Percent Hospital Reporting Days` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Adult COVID-19 Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Pediatric COVID-19 Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Influenza Admissions` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Prevalent Influenza Hospitalizations` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Hospitalized Influenza ICU Patients` &lt;dbl&gt;,\n#   `Number Hospitals Reporting Inpatient Beds` &lt;dbl&gt;, …\n\ncolnames(cdcdata)\n\n [1] \"Week Ending Date\"                                                     \n [2] \"Geographic aggregation\"                                               \n [3] \"Hospital Reporting Days\"                                              \n [4] \"Percent Hospital Reporting Days\"                                      \n [5] \"Number Hospitals Reporting Adult COVID-19 Admissions\"                 \n [6] \"Number Hospitals Reporting Pediatric COVID-19 Admissions\"             \n [7] \"Number Hospitals Reporting Influenza Admissions\"                      \n [8] \"Number Hospitals Reporting Prevalent Influenza Hospitalizations\"      \n [9] \"Number Hospitals Reporting Hospitalized Influenza ICU Patients\"       \n[10] \"Number Hospitals Reporting Inpatient Beds\"                            \n[11] \"Number Hospitals Reporting ICU Beds\"                                  \n[12] \"Number Hospitals Reporting Inpatient Beds Occupied\"                   \n[13] \"Number Hospitals Reporting ICU Beds Occupied\"                         \n[14] \"Number Hospitals Reporting Percent Inpatient Bed Occupancy\"           \n[15] \"Number Hospitals Reporting Percent ICU Bed Occupancy\"                 \n[16] \"Number Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\"  \n[17] \"Number Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\" \n[18] \"Number Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\"        \n[19] \"Number Hospitals Reporting Percent Influenza ICU Bed Occupancy\"       \n[20] \"Number Hospitals Reporting Total COVID-19 Admissions\"                 \n[21] \"Number Hospitals Reporting Prevalent COVID-19 Hospitalizations\"       \n[22] \"Number Hospitals Reporting Hospitalized COVID-19 ICU Patients\"        \n[23] \"Weekly Average Adult COVID-19 Admissions\"                             \n[24] \"Weekly Total Adult COVID-19 Admissions\"                               \n[25] \"Weekly Average Pediatric COVID-19 Admissions\"                         \n[26] \"Weekly Total Pediatric COVID-19 Admissions\"                           \n[27] \"Weekly Average COVID-19 Admissions\"                                   \n[28] \"Weekly Total COVID-19 Admissions\"                                     \n[29] \"Weekly Average Influenza Admissions\"                                  \n[30] \"Weekly Total Influenza Admissions\"                                    \n[31] \"Weekly Average Prevalent COVID-19 Hospitalizations\"                   \n[32] \"Weekly Average Prevalent Influenza Hospitalizations\"                  \n[33] \"Weekly Average Hospitalized COVID-19 ICU Patients\"                    \n[34] \"Weekly Average Hospitalized Influenza ICU Patients\"                   \n[35] \"Weekly Average Inpatient Beds\"                                        \n[36] \"Weekly Average ICU Beds\"                                              \n[37] \"Weekly Average Inpatient Beds Occupied\"                               \n[38] \"Weekly Average ICU Beds Occupied\"                                     \n[39] \"Weekly Average Percent Inpatient Bed Occupancy\"                       \n[40] \"Weekly Average Percent ICU Bed Occupancy\"                             \n[41] \"Weekly Average Percent COVID-19 Inpatient Bed Occupancy\"              \n[42] \"Weekly Average Percent Influenza Inpatient Bed Occupancy\"             \n[43] \"Weekly Average Percent COVID-19 ICU Bed Occupancy\"                    \n[44] \"Weekly Average Percent Influenza ICU Bed Occupancy\"                   \n[45] \"Percent Adult COVID-19 Admissions\"                                    \n[46] \"Percent Pediatric COVID-19 Admissions\"                                \n[47] \"Percent Hospitals Reporting Adult COVID-19 Admissions\"                \n[48] \"Percent Hospitals Reporting Pediatric COVID-19 Admissions\"            \n[49] \"Percent Hospitals Reporting Influenza Admissions\"                     \n[50] \"Percent Hospitals Reporting Prevalent Influenza Hospitalizations\"     \n[51] \"Percent Hospitals Reporting Hospitalized Influenza ICU Patients\"      \n[52] \"Percent Hospitals Reporting Inpatient Beds\"                           \n[53] \"Percent Hospitals Reporting ICU Beds\"                                 \n[54] \"Percent Hospitals Reporting Inpatient Beds Occupied\"                  \n[55] \"Percent Hospitals Reporting ICU Beds Occupied\"                        \n[56] \"Percent Hospitals Reporting Percent Inpatient Bed Occupancy\"          \n[57] \"Percent Hospitals Reporting Percent ICU Bed Occupancy\"                \n[58] \"Percent Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\" \n[59] \"Percent Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\"\n[60] \"Percent Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\"       \n[61] \"Percent Hospitals Reporting Percent Influenza ICU Bed Occupancy\"      \n[62] \"Percent Hospitals Reporting Total COVID-19 Admissions\"                \n[63] \"Percent Hospitals Reporting Prevalent COVID-19 Hospitalizations\"      \n[64] \"Percent Hospitals Reporting Hospitalized COVID-19 ICU Patients\"       \n\n\n\n# Check for missing values\nsummary(cdcdata)\n\n Week Ending Date     Geographic aggregation Hospital Reporting Days\n Min.   :2020-08-08   Length:11571           Min.   :    0          \n 1st Qu.:2021-07-24   Class :character       1st Qu.:  229          \n Median :2022-07-16   Mode  :character       Median :  497          \n Mean   :2022-07-16                          Mean   : 1155          \n 3rd Qu.:2023-07-08                          3rd Qu.:  832          \n Max.   :2024-06-22                          Max.   :34044          \n                                                                    \n Percent Hospital Reporting Days\n Min.   :0.0000                 \n 1st Qu.:0.9600                 \n Median :0.9800                 \n Mean   :0.9403                 \n 3rd Qu.:1.0000                 \n Max.   :1.0000                 \n                                \n Number Hospitals Reporting Adult COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  33.0                                      \n Median :  72.0                                      \n Mean   : 166.4                                      \n 3rd Qu.: 121.0                                      \n Max.   :4887.0                                      \n                                                     \n Number Hospitals Reporting Pediatric COVID-19 Admissions\n Min.   :   0.0                                          \n 1st Qu.:  33.0                                          \n Median :  72.0                                          \n Mean   : 165.5                                          \n 3rd Qu.: 120.0                                          \n Max.   :4887.0                                          \n                                                         \n Number Hospitals Reporting Influenza Admissions\n Min.   :   0.0                                 \n 1st Qu.:  26.0                                 \n Median :  62.0                                 \n Mean   : 152.8                                 \n 3rd Qu.: 114.0                                 \n Max.   :4887.0                                 \n                                                \n Number Hospitals Reporting Prevalent Influenza Hospitalizations\n Min.   :   0.0                                                 \n 1st Qu.:  26.0                                                 \n Median :  62.0                                                 \n Mean   : 152.8                                                 \n 3rd Qu.: 114.0                                                 \n Max.   :4887.0                                                 \n                                                                \n Number Hospitals Reporting Hospitalized Influenza ICU Patients\n Min.   :   0.0                                                \n 1st Qu.:  26.0                                                \n Median :  62.0                                                \n Mean   : 152.8                                                \n 3rd Qu.: 114.0                                                \n Max.   :4887.0                                                \n                                                               \n Number Hospitals Reporting Inpatient Beds Number Hospitals Reporting ICU Beds\n Min.   :   0.0                            Min.   :   0.0                     \n 1st Qu.:  33.0                            1st Qu.:  33.0                     \n Median :  72.0                            Median :  72.0                     \n Mean   : 165.8                            Mean   : 166.2                     \n 3rd Qu.: 121.0                            3rd Qu.: 121.0                     \n Max.   :4883.0                            Max.   :4887.0                     \n                                                                              \n Number Hospitals Reporting Inpatient Beds Occupied\n Min.   :   0.0                                    \n 1st Qu.:  33.0                                    \n Median :  72.0                                    \n Mean   : 165.8                                    \n 3rd Qu.: 121.0                                    \n Max.   :4883.0                                    \n                                                   \n Number Hospitals Reporting ICU Beds Occupied\n Min.   :   0.0                              \n 1st Qu.:  33.0                              \n Median :  71.0                              \n Mean   : 164.8                              \n 3rd Qu.: 116.0                              \n Max.   :4879.0                              \n                                             \n Number Hospitals Reporting Percent Inpatient Bed Occupancy\n Min.   :   0.0                                            \n 1st Qu.:  33.0                                            \n Median :  72.0                                            \n Mean   : 165.8                                            \n 3rd Qu.: 121.0                                            \n Max.   :4883.0                                            \n                                                           \n Number Hospitals Reporting Percent ICU Bed Occupancy\n Min.   :   0.0                                      \n 1st Qu.:  33.0                                      \n Median :  71.0                                      \n Mean   : 164.8                                      \n 3rd Qu.: 116.0                                      \n Max.   :4879.0                                      \n                                                     \n Number Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\n Min.   :   0.0                                                     \n 1st Qu.:  33.0                                                     \n Median :  71.0                                                     \n Mean   : 165.4                                                     \n 3rd Qu.: 121.0                                                     \n Max.   :4883.0                                                     \n                                                                    \n Number Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\n Min.   :   0.0                                                      \n 1st Qu.:  26.0                                                      \n Median :  62.0                                                      \n Mean   : 152.4                                                      \n 3rd Qu.: 114.0                                                      \n Max.   :4883.0                                                      \n                                                                     \n Number Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\n Min.   :   0.0                                               \n 1st Qu.:  33.0                                               \n Median :  72.0                                               \n Mean   : 165.9                                               \n 3rd Qu.: 121.0                                               \n Max.   :4887.0                                               \n                                                              \n Number Hospitals Reporting Percent Influenza ICU Bed Occupancy\n Min.   :   0.0                                                \n 1st Qu.:  26.0                                                \n Median :  62.0                                                \n Mean   : 152.7                                                \n 3rd Qu.: 114.0                                                \n Max.   :4887.0                                                \n                                                               \n Number Hospitals Reporting Total COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  33.0                                      \n Median :  72.0                                      \n Mean   : 166.5                                      \n 3rd Qu.: 121.0                                      \n Max.   :4887.0                                      \n                                                     \n Number Hospitals Reporting Prevalent COVID-19 Hospitalizations\n Min.   :   0.0                                                \n 1st Qu.:  33.0                                                \n Median :  72.0                                                \n Mean   : 166.1                                                \n 3rd Qu.: 121.0                                                \n Max.   :4887.0                                                \n                                                               \n Number Hospitals Reporting Hospitalized COVID-19 ICU Patients\n Min.   :   0.0                                               \n 1st Qu.:  33.0                                               \n Median :  72.0                                               \n Mean   : 165.5                                               \n 3rd Qu.: 121.0                                               \n Max.   :4887.0                                               \n                                                              \n Weekly Average Adult COVID-19 Admissions\n Min.   :    0.0                         \n 1st Qu.:   10.0                         \n Median :   33.0                         \n Mean   :  165.4                         \n 3rd Qu.:   93.0                         \n Max.   :20287.0                         \n NA's   :177                             \n Weekly Total Adult COVID-19 Admissions\n Min.   :     0.0                      \n 1st Qu.:    73.0                      \n Median :   229.5                      \n Mean   :  1157.7                      \n 3rd Qu.:   653.0                      \n Max.   :142007.0                      \n NA's   :177                           \n Weekly Average Pediatric COVID-19 Admissions\n Min.   :  0.000                             \n 1st Qu.:  0.000                             \n Median :  1.000                             \n Mean   :  5.648                             \n 3rd Qu.:  3.000                             \n Max.   :907.000                             \n NA's   :186                                 \n Weekly Total Pediatric COVID-19 Admissions Weekly Average COVID-19 Admissions\n Min.   :   0.00                            Min.   :    0.0                   \n 1st Qu.:   2.00                            1st Qu.:   11.0                   \n Median :   9.00                            Median :   34.0                   \n Mean   :  39.74                            Mean   :  171.1                   \n 3rd Qu.:  23.00                            3rd Qu.:   97.0                   \n Max.   :6351.00                            Max.   :21194.0                   \n NA's   :186                                NA's   :177                       \n Weekly Total COVID-19 Admissions Weekly Average Influenza Admissions\n Min.   :     0                   Min.   :   0.00                    \n 1st Qu.:    78                   1st Qu.:   0.00                    \n Median :   240                   Median :   1.00                    \n Mean   :  1197                   Mean   :  14.42                    \n 3rd Qu.:   676                   3rd Qu.:   5.00                    \n Max.   :148358                   Max.   :3697.00                    \n NA's   :177                      NA's   :692                        \n Weekly Total Influenza Admissions\n Min.   :    0.0                  \n 1st Qu.:    1.0                  \n Median :    9.0                  \n Mean   :  101.1                  \n 3rd Qu.:   37.0                  \n Max.   :25882.0                  \n NA's   :692                      \n Weekly Average Prevalent COVID-19 Hospitalizations\n Min.   :     0.0                                  \n 1st Qu.:    68.0                                  \n Median :   222.0                                  \n Mean   :  1202.0                                  \n 3rd Qu.:   654.8                                  \n Max.   :144095.0                                  \n NA's   :177                                       \n Weekly Average Prevalent Influenza Hospitalizations\n Min.   :    0.0                                    \n 1st Qu.:    1.0                                    \n Median :    7.0                                    \n Mean   :   75.9                                    \n 3rd Qu.:   29.0                                    \n Max.   :15926.0                                    \n NA's   :700                                        \n Weekly Average Hospitalized COVID-19 ICU Patients\n Min.   :    0.0                                  \n 1st Qu.:   10.0                                  \n Median :   38.0                                  \n Mean   :  247.6                                  \n 3rd Qu.:  130.0                                  \n Max.   :26475.0                                  \n NA's   :188                                      \n Weekly Average Hospitalized Influenza ICU Patients\n Min.   :   0.00                                   \n 1st Qu.:   0.00                                   \n Median :   1.00                                   \n Mean   :  12.06                                   \n 3rd Qu.:   4.00                                   \n Max.   :2334.00                                   \n NA's   :692                                       \n Weekly Average Inpatient Beds Weekly Average ICU Beds\n Min.   :     7                Min.   :     0         \n 1st Qu.:  2693                1st Qu.:   366         \n Median :  7915                Median :  1192         \n Mean   : 23555                Mean   :  3538         \n 3rd Qu.: 16131                3rd Qu.:  2533         \n Max.   :711091                Max.   :113184         \n NA's   :177                   NA's   :177            \n Weekly Average Inpatient Beds Occupied Weekly Average ICU Beds Occupied\n Min.   :     7                         Min.   :    0                   \n 1st Qu.:  1992                         1st Qu.:  251                   \n Median :  5571                         Median :  812                   \n Mean   : 17548                         Mean   : 2497                   \n 3rd Qu.: 11709                         3rd Qu.: 1642                   \n Max.   :538595                         Max.   :82717                   \n NA's   :177                            NA's   :192                     \n Weekly Average Percent Inpatient Bed Occupancy\n Min.   :0.3100                                \n 1st Qu.:0.6700                                \n Median :0.7300                                \n Mean   :0.7203                                \n 3rd Qu.:0.7900                                \n Max.   :1.0300                                \n NA's   :177                                   \n Weekly Average Percent ICU Bed Occupancy\n Min.   :0.0000                          \n 1st Qu.:0.6300                          \n Median :0.7000                          \n Mean   :0.6843                          \n 3rd Qu.:0.7600                          \n Max.   :1.0000                          \n NA's   :200                             \n Weekly Average Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.00000                                        \n 1st Qu.:0.01000                                        \n Median :0.03000                                        \n Mean   :0.04447                                        \n 3rd Qu.:0.05000                                        \n Max.   :0.35000                                        \n NA's   :177                                            \n Weekly Average Percent Influenza Inpatient Bed Occupancy\n Min.   :0.0000                                          \n 1st Qu.:0.0000                                          \n Median :0.0000                                          \n Mean   :0.0024                                          \n 3rd Qu.:0.0000                                          \n Max.   :0.0700                                          \n NA's   :695                                             \n Weekly Average Percent COVID-19 ICU Bed Occupancy\n Min.   :0.00000                                  \n 1st Qu.:0.01000                                  \n Median :0.03000                                  \n Mean   :0.06093                                  \n 3rd Qu.:0.08000                                  \n Max.   :0.68000                                  \n NA's   :196                                      \n Weekly Average Percent Influenza ICU Bed Occupancy\n Min.   :0.0000                                    \n 1st Qu.:0.0000                                    \n Median :0.0000                                    \n Mean   :0.0027                                    \n 3rd Qu.:0.0000                                    \n Max.   :0.0900                                    \n NA's   :730                                       \n Percent Adult COVID-19 Admissions Percent Pediatric COVID-19 Admissions\n Min.   :0.0000                    Min.   :0.0000                       \n 1st Qu.:0.9500                    1st Qu.:0.0200                       \n Median :0.9700                    Median :0.0300                       \n Mean   :0.9544                    Mean   :0.0456                       \n 3rd Qu.:0.9800                    3rd Qu.:0.0500                       \n Max.   :1.0000                    Max.   :1.0000                       \n NA's   :449                       NA's   :449                          \n Percent Hospitals Reporting Adult COVID-19 Admissions\n Min.   :0.0000                                       \n 1st Qu.:0.9700                                       \n Median :0.9900                                       \n Mean   :0.9488                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Pediatric COVID-19 Admissions\n Min.   :0.0000                                           \n 1st Qu.:0.9700                                           \n Median :0.9900                                           \n Mean   :0.9431                                           \n 3rd Qu.:1.0000                                           \n Max.   :1.0000                                           \n                                                          \n Percent Hospitals Reporting Influenza Admissions\n Min.   :0.0000                                  \n 1st Qu.:0.9500                                  \n Median :0.9800                                  \n Mean   :0.8755                                  \n 3rd Qu.:1.0000                                  \n Max.   :1.0000                                  \n                                                 \n Percent Hospitals Reporting Prevalent Influenza Hospitalizations\n Min.   :0.0000                                                  \n 1st Qu.:0.9500                                                  \n Median :0.9800                                                  \n Mean   :0.8757                                                  \n 3rd Qu.:1.0000                                                  \n Max.   :1.0000                                                  \n                                                                 \n Percent Hospitals Reporting Hospitalized Influenza ICU Patients\n Min.   :0.0000                                                 \n 1st Qu.:0.9500                                                 \n Median :0.9800                                                 \n Mean   :0.8755                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Inpatient Beds\n Min.   :0.0000                            \n 1st Qu.:0.9600                            \n Median :0.9800                            \n Mean   :0.9451                            \n 3rd Qu.:1.0000                            \n Max.   :1.0000                            \n                                           \n Percent Hospitals Reporting ICU Beds\n Min.   :0.0000                      \n 1st Qu.:0.9700                      \n Median :0.9900                      \n Mean   :0.9479                      \n 3rd Qu.:1.0000                      \n Max.   :1.0000                      \n                                     \n Percent Hospitals Reporting Inpatient Beds Occupied\n Min.   :0.0000                                     \n 1st Qu.:0.9600                                     \n Median :0.9800                                     \n Mean   :0.9451                                     \n 3rd Qu.:1.0000                                     \n Max.   :1.0000                                     \n                                                    \n Percent Hospitals Reporting ICU Beds Occupied\n Min.   :0.0000                               \n 1st Qu.:0.9600                               \n Median :0.9800                               \n Mean   :0.9383                               \n 3rd Qu.:1.0000                               \n Max.   :1.0000                               \n                                              \n Percent Hospitals Reporting Percent Inpatient Bed Occupancy\n Min.   :0.0000                                             \n 1st Qu.:0.9600                                             \n Median :0.9800                                             \n Mean   :0.9451                                             \n 3rd Qu.:1.0000                                             \n Max.   :1.0000                                             \n                                                            \n Percent Hospitals Reporting Percent ICU Bed Occupancy\n Min.   :0.0000                                       \n 1st Qu.:0.9600                                       \n Median :0.9800                                       \n Mean   :0.9383                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.000                                                       \n 1st Qu.:0.960                                                       \n Median :0.980                                                       \n Mean   :0.943                                                       \n 3rd Qu.:1.000                                                       \n Max.   :1.000                                                       \n                                                                     \n Percent Hospitals Reporting Percent Influenza Inpatient Bed Occupancy\n Min.   :0.0000                                                       \n 1st Qu.:0.9500                                                       \n Median :0.9800                                                       \n Mean   :0.8738                                                       \n 3rd Qu.:1.0000                                                       \n Max.   :1.0000                                                       \n                                                                      \n Percent Hospitals Reporting Percent COVID-19 ICU Bed Occupancy\n Min.   :0.0000                                                \n 1st Qu.:0.9700                                                \n Median :0.9900                                                \n Mean   :0.9456                                                \n 3rd Qu.:1.0000                                                \n Max.   :1.0000                                                \n                                                               \n Percent Hospitals Reporting Percent Influenza ICU Bed Occupancy\n Min.   :0.0000                                                 \n 1st Qu.:0.9500                                                 \n Median :0.9800                                                 \n Mean   :0.8753                                                 \n 3rd Qu.:1.0000                                                 \n Max.   :1.0000                                                 \n                                                                \n Percent Hospitals Reporting Total COVID-19 Admissions\n Min.   :0.0000                                       \n 1st Qu.:0.9700                                       \n Median :0.9900                                       \n Mean   :0.9489                                       \n 3rd Qu.:1.0000                                       \n Max.   :1.0000                                       \n                                                      \n Percent Hospitals Reporting Prevalent COVID-19 Hospitalizations\n Min.   :0.000                                                  \n 1st Qu.:0.970                                                  \n Median :0.990                                                  \n Mean   :0.947                                                  \n 3rd Qu.:1.000                                                  \n Max.   :1.000                                                  \n                                                                \n Percent Hospitals Reporting Hospitalized COVID-19 ICU Patients\n Min.   :0.0000                                                \n 1st Qu.:0.9700                                                \n Median :0.9800                                                \n Mean   :0.9429                                                \n 3rd Qu.:1.0000                                                \n Max.   :1.0000                                                \n                                                               \n\n\nNext, I will process the data: Handle any missing values or non-standard codes and select relevant columns for analysis.\n\n# Convert date column to Date type\ncdcdata &lt;- cdcdata %&gt;%\n  mutate(`Week Ending Date` = as.Date(`Week Ending Date`, format = \"%Y-%m-%d\")) %&gt;%\n  mutate_all(~ifelse(. == 999, NA, .))\n\n# Select relevant columns for analysis\nselected_data &lt;- cdcdata %&gt;%\n  select(`Week Ending Date`, `Geographic aggregation`, `Number Hospitals Reporting Adult COVID-19 Admissions`, `Weekly Average Percent COVID-19 Inpatient Bed Occupancy`, `Weekly Average COVID-19 Admissions`, `Percent Hospitals Reporting Inpatient Beds Occupied`, `Weekly Average Inpatient Beds Occupied`, `Weekly Average ICU Beds Occupied`)\n\n# Inspect the cleaned data\nhead(selected_data)\n\n# A tibble: 6 × 8\n  `Week Ending Date` `Geographic aggregation` Number Hospitals Reporting Adult…¹\n               &lt;dbl&gt; &lt;chr&gt;                                                 &lt;dbl&gt;\n1              18482 AL                                                       60\n2              18489 AL                                                       66\n3              18496 AL                                                       78\n4              18503 AL                                                       90\n5              18510 AL                                                       94\n6              18517 AL                                                       94\n# ℹ abbreviated name: ¹​`Number Hospitals Reporting Adult COVID-19 Admissions`\n# ℹ 5 more variables:\n#   `Weekly Average Percent COVID-19 Inpatient Bed Occupancy` &lt;dbl&gt;,\n#   `Weekly Average COVID-19 Admissions` &lt;dbl&gt;,\n#   `Percent Hospitals Reporting Inpatient Beds Occupied` &lt;dbl&gt;,\n#   `Weekly Average Inpatient Beds Occupied` &lt;dbl&gt;,\n#   `Weekly Average ICU Beds Occupied` &lt;dbl&gt;\n\nsummary(selected_data)\n\n Week Ending Date Geographic aggregation\n Min.   :18482    Length:11571          \n 1st Qu.:18832    Class :character      \n Median :19189    Mode  :character      \n Mean   :19189                          \n 3rd Qu.:19546                          \n Max.   :19896                          \n                                        \n Number Hospitals Reporting Adult COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  33.0                                      \n Median :  72.0                                      \n Mean   : 166.4                                      \n 3rd Qu.: 121.0                                      \n Max.   :4887.0                                      \n                                                     \n Weekly Average Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.00000                                        \n 1st Qu.:0.01000                                        \n Median :0.03000                                        \n Mean   :0.04447                                        \n 3rd Qu.:0.05000                                        \n Max.   :0.35000                                        \n NA's   :177                                            \n Weekly Average COVID-19 Admissions\n Min.   :    0.0                   \n 1st Qu.:   11.0                   \n Median :   34.0                   \n Mean   :  170.9                   \n 3rd Qu.:   97.0                   \n Max.   :21194.0                   \n NA's   :179                       \n Percent Hospitals Reporting Inpatient Beds Occupied\n Min.   :0.0000                                     \n 1st Qu.:0.9600                                     \n Median :0.9800                                     \n Mean   :0.9451                                     \n 3rd Qu.:1.0000                                     \n Max.   :1.0000                                     \n                                                    \n Weekly Average Inpatient Beds Occupied Weekly Average ICU Beds Occupied\n Min.   :     7                         Min.   :    0                   \n 1st Qu.:  1992                         1st Qu.:  251                   \n Median :  5571                         Median :  812                   \n Mean   : 17548                         Mean   : 2497                   \n 3rd Qu.: 11709                         3rd Qu.: 1642                   \n Max.   :538595                         Max.   :82717                   \n NA's   :177                            NA's   :195                     \n\n\n\n\nExploratory Data Analysis\nIn this section, I will perform some exploratory/descriptive analysis on this cleaned dataset.\n\n# Summary table of key variables\nsummary(selected_data)\n\n Week Ending Date Geographic aggregation\n Min.   :18482    Length:11571          \n 1st Qu.:18832    Class :character      \n Median :19189    Mode  :character      \n Mean   :19189                          \n 3rd Qu.:19546                          \n Max.   :19896                          \n                                        \n Number Hospitals Reporting Adult COVID-19 Admissions\n Min.   :   0.0                                      \n 1st Qu.:  33.0                                      \n Median :  72.0                                      \n Mean   : 166.4                                      \n 3rd Qu.: 121.0                                      \n Max.   :4887.0                                      \n                                                     \n Weekly Average Percent COVID-19 Inpatient Bed Occupancy\n Min.   :0.00000                                        \n 1st Qu.:0.01000                                        \n Median :0.03000                                        \n Mean   :0.04447                                        \n 3rd Qu.:0.05000                                        \n Max.   :0.35000                                        \n NA's   :177                                            \n Weekly Average COVID-19 Admissions\n Min.   :    0.0                   \n 1st Qu.:   11.0                   \n Median :   34.0                   \n Mean   :  170.9                   \n 3rd Qu.:   97.0                   \n Max.   :21194.0                   \n NA's   :179                       \n Percent Hospitals Reporting Inpatient Beds Occupied\n Min.   :0.0000                                     \n 1st Qu.:0.9600                                     \n Median :0.9800                                     \n Mean   :0.9451                                     \n 3rd Qu.:1.0000                                     \n Max.   :1.0000                                     \n                                                    \n Weekly Average Inpatient Beds Occupied Weekly Average ICU Beds Occupied\n Min.   :     7                         Min.   :    0                   \n 1st Qu.:  1992                         1st Qu.:  251                   \n Median :  5571                         Median :  812                   \n Mean   : 17548                         Mean   : 2497                   \n 3rd Qu.: 11709                         3rd Qu.: 1642                   \n Max.   :538595                         Max.   :82717                   \n NA's   :177                            NA's   :195                     \n\n\n\n# Plot the distribution of Weekly Average COVID-19 Admissions\nggplot(selected_data, aes(x = `Weekly Average COVID-19 Admissions`)) +\n  geom_histogram(binwidth = 500, fill = \"blue\", color = \"blue\") +\n  labs(title = \"Distribution of Weekly Average COVID-19 Admissions\", x = \"Weekly Average COVID-19 Admissions\", y = \"Frequency\")\n\nWarning: Removed 179 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThis histogram shows the distribution of Weekly Average COVID-19 Admissions across the different jurisdictions. The majority of jurisdictions report fewer than 2000 admissions per week, with a long tail indicating that a few jurisdictions have significantly higher weekly admissions.\nI can see a very high frequency of lower values with a significant right skew. Most of the data points are clustered around the lower range of weekly average COVID-19 admissions, with a few outliers having very high values. This indicates that most jurisdictions have relatively low weekly average COVID-19 admissions, while a few have extremely high numbers.\n\n# Plot the distribution of Weekly Average Inpatient Beds Occupied\nggplot(selected_data, aes(x = `Weekly Average Inpatient Beds Occupied`)) +\n  geom_histogram(binwidth = 500, fill = \"black\", color = \"red\") +\n  labs(title = \"Distribution of Weekly Average Inpatient Beds Occupied\", x = \"Weekly Average Inpatient Beds Occupied\", y = \"Frequency\")\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThis histogram shows the distribution of Weekly Average Inpatient Beds Occupied. Similar to the previous plot, most jurisdictions have fewer than 2000 occupied inpatient beds per week, with a few jurisdictions having significantly higher numbers.\nSimilar to the previous histogram, this one also exhibits a strong right skew. Most jurisdictions have a lower number of weekly average inpatient beds occupied, with a few outliers at the higher end. The concentration of data points at the lower end suggests that most regions did not see a large number of inpatient beds occupied weekly.\n\n# Plot the distribution of Weekly Average ICU Beds Occupied\nggplot(selected_data, aes(x = `Weekly Average ICU Beds Occupied`)) +\n  geom_histogram(binwidth = 500, fill = \"lightblue\", color = \"lightblue\") +\n  labs(title = \"Distribution of Weekly Average ICU Beds Occupied\", x = \"Weekly Average ICU Beds Occupied\", y = \"Frequency\")\n\nWarning: Removed 195 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThis histogram shows the distribution of Weekly Average ICU Beds Occupied. The distribution is skewed towards lower numbers, with most jurisdictions reporting fewer than 500 ICU beds occupied per week.\nThis histogram also shows a right-skewed distribution, indicating that the majority of jurisdictions have a lower weekly average of ICU beds occupied. The distribution is similar to that of inpatient beds but with ICU-specific data.\n\n# Summary of categorical variable 'Geographic aggregation'\ngeo_summary &lt;- selected_data %&gt;%\n  group_by(`Geographic aggregation`) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count))\n\ngeo_summary\n\n# A tibble: 57 × 2\n   `Geographic aggregation` count\n   &lt;chr&gt;                    &lt;int&gt;\n 1 AK                         203\n 2 AL                         203\n 3 AR                         203\n 4 AS                         203\n 5 AZ                         203\n 6 CA                         203\n 7 CO                         203\n 8 CT                         203\n 9 DC                         203\n10 DE                         203\n# ℹ 47 more rows\n\n\nThis summary table shows the count of data points for each Geographic aggregation. This helps us understand the distribution of the data across different jurisdictions.\n\n# Summary of categorical variable 'Geographic aggregation'\ngeo_summary &lt;- selected_data %&gt;%\n  group_by(`Geographic aggregation`) %&gt;%\n  summarise(count = n()) %&gt;%\n  arrange(desc(count))\n\n# Filter for top 5 and bottom 5 states by count\ntop_5_states &lt;- geo_summary %&gt;% top_n(5, count)\nbottom_5_states &lt;- geo_summary %&gt;% top_n(-5, count)\n\n# Combine top and bottom 5 states\nselected_states &lt;- bind_rows(top_5_states, bottom_5_states)\n\n# Filter the data for these states\nfiltered_data &lt;- selected_data %&gt;% \n  filter(`Geographic aggregation` %in% selected_states$`Geographic aggregation`)\n\n# Plot the percentage of data points per Geographic aggregation for selected states\nggplot(filtered_data, aes(x = reorder(`Geographic aggregation`, -`Weekly Average COVID-19 Admissions`), y = `Weekly Average COVID-19 Admissions`)) +\n  geom_bar(stat = \"identity\", fill = \"orange\") +\n  coord_flip() +\n  labs(title = \"Weekly Average COVID-19 Admissions for Selected States\", x = \"Geographic Aggregation\", y = \"Weekly Average COVID-19 Admissions\")\n\nWarning: Removed 179 rows containing missing values (`position_stack()`).\n\n\n\n\n\nThis bar plot shows the Weekly Average COVID-19 Admissions for the top 5 and bottom 5 states. This helps us identify states with the highest and lowest average admissions.\nThis bar plot depicts the weekly average COVID-19 admissions for selected states. The plot highlights the stark contrast between states with very high and very low average COVID-19 admissions. It visually demonstrates that while most states have low to moderate admissions, a few have extremely high numbers.\n\n# Create boxplot for Weekly Average Percent COVID-19 Inpatient Bed Occupancy\nggplot(filtered_data, aes(x = `Geographic aggregation`, y = `Weekly Average Percent COVID-19 Inpatient Bed Occupancy`)) +\n  geom_boxplot(fill = \"lightblue\", color = \"darkblue\") +\n  coord_flip() +\n  labs(title = \"Boxplot of Weekly Average Percent COVID-19 Inpatient Bed Occupancy by State\", x = \"Geographic Aggregation\", y = \"Weekly Average Percent COVID-19 Inpatient Bed Occupancy\")\n\nWarning: Removed 177 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThis boxplot shows the distribution of Weekly Average Percent COVID-19 Inpatient Bed Occupancy across the selected states. It provides insights into the central tendency and variability of bed occupancy in these states.\nThis boxplot reveals the variation in the weekly average percent COVID-19 inpatient bed occupancy across different states. Some states have a higher median occupancy rate and more spread out data points, indicating greater variability in bed occupancy percentages.\n\n# Create boxplot for Percent Hospitals Reporting Inpatient Beds Occupied\nggplot(filtered_data, aes(x = `Geographic aggregation`, y = `Percent Hospitals Reporting Inpatient Beds Occupied`)) +\n  geom_boxplot(fill = \"lightgreen\", color = \"darkgreen\") +\n  coord_flip() +\n  labs(title = \"Boxplot of Percent Hospitals Reporting Inpatient Beds Occupied by State\", x = \"Geographic Aggregation\", y = \"Percent Hospitals Reporting Inpatient Beds Occupied\")\n\n\n\n\nThis boxplot shows the distribution of Percent Hospitals Reporting Inpatient Beds Occupied across the selected states. It helps us understand the proportion of hospitals reporting occupied beds in each state.\nThis boxplot shows the percent of hospitals reporting inpatient beds occupied across different states. Most states have a high percentage of hospitals reporting inpatient beds occupied, close to 100%, with some variability. This suggests that hospital bed occupancy reporting is consistent across most jurisdictions.\n\n# Create boxplot for Weekly Average Inpatient Beds Occupied\nggplot(filtered_data, aes(x = `Geographic aggregation`, y = `Weekly Average Inpatient Beds Occupied`)) +\n  geom_boxplot(fill = \"lightcoral\", color = \"darkred\") +\n  coord_flip() +\n  labs(title = \"Boxplot of Weekly Average Inpatient Beds Occupied by State\", x = \"Geographic Aggregation\", y = \"Weekly Average Inpatient Beds Occupied\")\n\nWarning: Removed 177 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThis boxplot shows the distribution of Weekly Average Inpatient Beds Occupied across the selected states. It provides insights into the central tendency and variability of inpatient bed occupancy in these states.\nThis boxplot indicates the weekly average number of inpatient beds occupied by state. There is a noticeable variation between states, with some having a significantly higher number of occupied beds on average. The presence of outliers suggests that certain states experienced unusually high occupancy levels.\n\n# Create boxplot for Weekly Average ICU Beds Occupied\nggplot(filtered_data, aes(x = `Geographic aggregation`, y = `Weekly Average ICU Beds Occupied`)) +\n  geom_boxplot(fill = \"lightpink\", color = \"darkred\") +\n  coord_flip() +\n  labs(title = \"Boxplot of Weekly Average ICU Beds Occupied by State\", x = \"Geographic Aggregation\", y = \"Weekly Average ICU Beds Occupied\")\n\nWarning: Removed 195 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nThis boxplot shows the distribution of Weekly Average ICU Beds Occupied across the selected states. It provides insights into the central tendency and variability of ICU bed occupancy in these states.\nSimilar to the inpatient beds occupied boxplot, this one shows the weekly average number of ICU beds occupied by state. The variation among states is evident, with some states having higher averages and others lower. The outliers indicate states with exceptionally high ICU bed occupancy.\n\n\nConclusion:\nOverall, these plots illustrate the significant variability in COVID-19-related hospital metrics across different jurisdictions. The data is heavily skewed in many cases, indicating that while most areas experienced moderate levels, a few regions faced extremely high demands on their healthcare systems. The boxplots highlight the differences in hospital bed occupancy and reporting consistency across states, providing valuable insights into the impact of COVID-19 on healthcare resources.\nThis section contributed by Sri Lakshmi Sudha Ganni"
  }
]